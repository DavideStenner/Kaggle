{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005447,
     "end_time": "2021-01-28T19:21:38.264695",
     "exception": false,
     "start_time": "2021-01-28T19:21:38.259248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PARAMETER\n",
    "Change model path and the number of model to import in range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T19:21:38.279785Z",
     "iopub.status.busy": "2021-01-28T19:21:38.279137Z",
     "iopub.status.idle": "2021-01-28T19:21:39.829148Z",
     "shell.execute_reply": "2021-01-28T19:21:39.828418Z"
    },
    "papermill": {
     "duration": 1.560144,
     "end_time": "2021-01-28T19:21:39.829367",
     "exception": false,
     "start_time": "2021-01-28T19:21:38.269223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f5aae272550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import lightgbm as lgb\n",
    "\n",
    "NUMBER_OF_MODELS = 1\n",
    "model_path = '../input/lgballdata/lgb-all-data'\n",
    "\n",
    "model = lgb.Booster(model_file = os.path.join(model_path, f'model.txt'))\n",
    "model.save_model(f'model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-28T19:21:39.850349Z",
     "iopub.status.busy": "2021-01-28T19:21:39.849242Z",
     "iopub.status.idle": "2021-01-28T19:21:39.854562Z",
     "shell.execute_reply": "2021-01-28T19:21:39.855275Z"
    },
    "papermill": {
     "duration": 0.020167,
     "end_time": "2021-01-28T19:21:39.855528",
     "exception": false,
     "start_time": "2021-01-28T19:21:39.835361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\"\"\"Greedy agent that chooses machine based on maximum expected payout\n",
    "\n",
    "Uses a trained decision tree model to consider the other player's movements\n",
    "in the expected payout.\n",
    "\n",
    "See my other kernel for methodology for generating training data:\n",
    "https://www.kaggle.com/lebroschar/generate-training-data\n",
    "\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "TRAIN_FEATS = [\n",
    "    'round_num', 'n_pulls_self', 'n_success_self', 'n_failure_self',\n",
    "    'discounted_cumulative_success', 'discounted_total_success', 'n_pulls_opp', 'n_pulls_tot',\n",
    "    'ratio_self', 'ratio_opp', 'est_1', 'est_2', 'est_3', 'repeat_opp',\n",
    "    'n_pulls_self_last_10', 'n_pulls_opp_last_10', 'ratio_self_last_10_selected'\n",
    "]\n",
    "\n",
    "sys.path.append(\"/kaggle_simulations/agent\")\n",
    "working_dir = \"/kaggle_simulations/agent\"\n",
    "\n",
    "\n",
    "class GreedyStrategy:\n",
    "    \"\"\"Implements strategy to maximize expected value\n",
    "\n",
    "    - Tracks estimated likelihood of payout ratio for each machine\n",
    "    - Tracks number of pulls on each machine\n",
    "    - Chooses machine based on maximum expected value\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, name, agent_num, n_machines):\n",
    "        \"\"\"Initialize and train decision tree model\n",
    "\n",
    "        Args:\n",
    "           name (str):   Name for the agent\n",
    "           agent_num (int):   Assigned player number\n",
    "           n_machines (int):   number of machines in the game\n",
    "        \n",
    "        \"\"\"\n",
    "        # Record inputs\n",
    "        self.name = name\n",
    "        self.agent_num = agent_num\n",
    "        self.n_machines = n_machines\n",
    "        \n",
    "        # Initialize distributions for all machines\n",
    "        self.n_pulls_self = np.array([0. for _ in range(n_machines)])\n",
    "        self.n_success_self = np.array([0. for _ in range(n_machines)])\n",
    "        self.n_failure_self = np.array([0. for _ in range(n_machines)])\n",
    "        self.discounted_cumulative_success = np.array([0. for _ in range(n_machines)])\n",
    "        self.discounted_total_success = np.array([0. for _ in range(n_machines)])\n",
    "        self.n_pulls_opp = np.array([0. for _ in range(n_machines)])\n",
    "        self.n_pulls_tot = np.array([0. for _ in range(n_machines)])\n",
    "        self.ratio_self = np.array([0. for _ in range(n_machines)])\n",
    "        self.ratio_opp = np.array([0. for _ in range(n_machines)])\n",
    "        self.est_1 = np.array([0. for _ in range(n_machines)])\n",
    "        self.est_2 = np.array([0. for _ in range(n_machines)])\n",
    "        self.est_3 = np.array([0. for _ in range(n_machines)])\n",
    "        self.repeat_opp = np.array([0. for _ in range(n_machines)])\n",
    "        self.n_pulls_self_last_10 = np.array([0. for _ in range(n_machines)])\n",
    "        self.n_pulls_opp_last_10 = np.array([0. for _ in range(n_machines)])\n",
    "        self.ratio_self_last_10_selected = np.array([0. for _ in range(n_machines)])\n",
    "        \n",
    "        # Track winnings\n",
    "        self.last_reward_count = 0\n",
    "\n",
    "        self.model_lgb = lgb.Booster(model_file = os.path.join(working_dir, f'model.txt'))\n",
    "\n",
    "        self.round = 0\n",
    "        self.n_fold = 1\n",
    "        \n",
    "        self.decay_rate = .97\n",
    "        \n",
    "        # Predict expected reward\n",
    "        features = np.zeros((self.n_machines, len(TRAIN_FEATS)))\n",
    "        \n",
    "        # Predict expected reward\n",
    "        self.predicts = self.mod_pred(features)\n",
    "        self.last_reward = 0\n",
    "        \n",
    "        self.last_action = [-1, -1]\n",
    "\n",
    "        #used for n_pulls_self_last_10, n_pulls_opp_last_10\n",
    "        self.self_memory, self.opp_memory = [], []\n",
    "\n",
    "        #used for ratio_self_last_10_selected\n",
    "        self.self_memory_per_action = [[] for _ in range(n_machines)]\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Choose machine based on maximum expected payout\n",
    "\n",
    "        Returns:\n",
    "           <result> (int):  index of machine to pull\n",
    "        \n",
    "        \"\"\"\n",
    "        est_return = self.predicts\n",
    "        max_return = np.max(est_return)\n",
    "        result = np.random.choice(np.where(\n",
    "            est_return >= max_return)[0])\n",
    "        \n",
    "        return int(result)\n",
    "    \n",
    "    def firstRound(self):\n",
    "        result = np.random.choice(range(self.n_machines), 1)\n",
    "        return int(result)\n",
    "    \n",
    "    def pred_lgb(self, obs):\n",
    "        pred = self.model_lgb.predict(obs)\n",
    "                \n",
    "        return pred\n",
    "    \n",
    "    def mod_pred(self, obs):\n",
    "        lgb_pred = self.pred_lgb(obs)\n",
    "        \n",
    "        return lgb_pred\n",
    "    \n",
    "    def updateDist(self, curr_total_reward, last_m_indices):\n",
    "        \"\"\"Updates estimated distribution of payouts\"\"\"\n",
    "        # Compute last reward\n",
    "        self.round += 1\n",
    "        \n",
    "        #compute last reward\n",
    "        self.last_reward = curr_total_reward - self.last_reward_count\n",
    "        self.last_reward_count = curr_total_reward\n",
    "\n",
    "        # Update number of pulls for both machines\n",
    "        m_index = last_m_indices[self.agent_num]        \n",
    "        opp_index = last_m_indices[1 - self.agent_num]\n",
    "        \n",
    "        #update memory information\n",
    "        self.self_memory_per_action[m_index] += [self.last_reward]\n",
    "\n",
    "        self.self_memory += [m_index]\n",
    "        self.opp_memory += [opp_index]\n",
    "\n",
    "        #numbers of pulls information\n",
    "        self.n_pulls_self[m_index] += 1\n",
    "        self.n_pulls_opp[opp_index] += 1\n",
    "        \n",
    "        #numbers of pulls total\n",
    "        self.n_pulls_tot[m_index] += 1\n",
    "        self.n_pulls_tot[opp_index] += 1\n",
    "        \n",
    "        # Update number of successes and failure\n",
    "        self.n_success_self[m_index] += self.last_reward\n",
    "        self.n_failure_self[m_index] += (1 - self.last_reward)\n",
    "        \n",
    "        #count discounted success\n",
    "        self.discounted_cumulative_success[m_index] += (\n",
    "                        self.last_reward * self.decay_rate ** self.n_pulls_tot[m_index]\n",
    "        )\n",
    "\n",
    "        self.discounted_total_success[m_index] = (\n",
    "                    self.n_success_self[m_index] * self.decay_rate ** self.n_pulls_tot[m_index]\n",
    "        )\n",
    "\n",
    "        self.ratio_self[m_index] = self.n_success_self[m_index]/self.n_pulls_self[m_index]\n",
    "        \n",
    "        #est 3\n",
    "        self.est_3[m_index] = (\n",
    "            self.ratio_self[m_index] * math.pow(self.decay_rate, self.n_pulls_tot[m_index])\n",
    "        )\n",
    "\n",
    "        #opponent repeat feature\n",
    "        if opp_index == self.last_action[1 - self.agent_num]:\n",
    "            self.repeat_opp[opp_index] += 1\n",
    "            \n",
    "        else:\n",
    "            self.repeat_opp[self.last_action[1 - self.agent_num]] = 0\n",
    "            \n",
    "            #new last action\n",
    "            self.last_action[1 - self.agent_num] = opp_index\n",
    "                \n",
    "        # Update predictions for chosen machines\n",
    "        obs = []\n",
    "        \n",
    "        for i, index in enumerate(range(self.n_machines)):\n",
    "            self.ratio_opp[index] = self.n_pulls_opp[index]/(1 + self.round)\n",
    "\n",
    "            self.est_1[index] = (\n",
    "                self.n_success_self[index] - self.n_failure_self[index] + \\\n",
    "                self.n_pulls_opp[index] - 1.5 * (self.n_pulls_opp[index]>0)\n",
    "            )/self.n_pulls_tot[index] if self.n_pulls_tot[index] != 0 else 0\n",
    "\n",
    "            self.est_2[index] = (\n",
    "                (self.n_success_self[index] - self.n_failure_self[index] + self.n_pulls_opp[index] - 1.5 * (self.n_pulls_opp[index]>0)\\\n",
    "                 + self.repeat_opp[index])/self.n_pulls_tot[index]\n",
    "            ) * math.pow(self.decay_rate, self.n_pulls_tot[index]) if self.n_pulls_tot[index] != 0 else 0\n",
    "            \n",
    "            #number of self/opp pulls in last 10 action features\n",
    "            self.n_pulls_self_last_10[index] = sum([x == index for x in self.self_memory[-10:]])\n",
    "            self.n_pulls_opp_last_10[index] = sum([x == index for x in self.opp_memory[-10:]])\n",
    "\n",
    "            #ratio of rewrd in last 10 times i select the action\n",
    "            replay_self_last_10 = self.self_memory_per_action[index][-10:]\n",
    "            \n",
    "            #if never taken then 0 else the ratio of last 10 pulls on this action\n",
    "            self.ratio_self_last_10_selected[index] = 0 if len(replay_self_last_10) == 0 else sum(replay_self_last_10)/len(replay_self_last_10)\n",
    "\n",
    "            obs += [\n",
    "                [\n",
    "                    self.round,\n",
    "                    self.n_pulls_self[index],\n",
    "                    self.n_success_self[index],\n",
    "                    self.n_failure_self[index],\n",
    "                    self.discounted_cumulative_success[index],\n",
    "                    self.discounted_total_success[index],\n",
    "                    self.n_pulls_opp[index],\n",
    "                    self.n_pulls_tot[index],\n",
    "                    self.ratio_self[index],\n",
    "                    self.ratio_opp[index],\n",
    "                    self.est_1[index],\n",
    "                    self.est_2[index],\n",
    "                    self.est_3[index],\n",
    "                    self.repeat_opp[index],\n",
    "                    self.n_pulls_self_last_10[index],\n",
    "                    self.n_pulls_opp_last_10[index],\n",
    "                    self.ratio_self_last_10_selected[index],\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "        self.predicts = self.mod_pred(obs)\n",
    "\n",
    "def agent(observation, configuration):\n",
    "    global curr_agent\n",
    "    \n",
    "    if observation.step == 0:\n",
    "        # Initialize agent\n",
    "        curr_agent = GreedyStrategy(\n",
    "            'Mr. Agent %i' % observation['agentIndex'],\n",
    "            observation['agentIndex'],\n",
    "            configuration['banditCount'])\n",
    "        return curr_agent.firstRound()\n",
    "            \n",
    "    else:\n",
    "        # Update payout ratio distribution with:\n",
    "        curr_agent.updateDist(observation['reward'], observation['lastActions'])\n",
    "\n",
    "        return curr_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T19:21:39.873170Z",
     "iopub.status.busy": "2021-01-28T19:21:39.872518Z",
     "iopub.status.idle": "2021-01-28T19:21:41.260204Z",
     "shell.execute_reply": "2021-01-28T19:21:41.259668Z"
    },
    "papermill": {
     "duration": 1.398411,
     "end_time": "2021-01-28T19:21:41.260382",
     "exception": false,
     "start_time": "2021-01-28T19:21:39.861971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.py\r\n",
      "model.txt\r\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz main.py.tar.gz main.py model.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.790978,
   "end_time": "2021-01-28T19:21:41.977200",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-28T19:21:32.186222",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
