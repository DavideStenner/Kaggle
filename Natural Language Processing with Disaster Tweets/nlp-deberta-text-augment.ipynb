{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T21:01:37.720227Z","iopub.status.busy":"2023-04-11T21:01:37.719802Z","iopub.status.idle":"2023-04-11T21:01:51.442437Z","shell.execute_reply":"2023-04-11T21:01:51.441111Z","shell.execute_reply.started":"2023-04-11T21:01:37.720189Z"},"trusted":true},"outputs":[],"source":["!pip install -q transformers sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-11T21:01:51.445910Z","iopub.status.busy":"2023-04-11T21:01:51.445540Z","iopub.status.idle":"2023-04-11T21:02:01.482278Z","shell.execute_reply":"2023-04-11T21:02:01.480828Z","shell.execute_reply.started":"2023-04-11T21:01:51.445854Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import logging\n","import gc\n","import transformers\n","\n","import pandas as pd\n","import numpy as np\n","import pytorch_lightning as pl\n","import torch.nn.functional as F\n","import plotly.express as px\n","\n","from transformers import AutoModel, AutoTokenizer\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import f1_score\n","from torch import nn\n","from glob import glob\n","\n","from torchmetrics.classification import BinaryAUROC, BinaryF1Score\n","\n","PATH_DATA = '/kaggle/input/nlp-get-started-cleaning-text-tr'\n","\n","SAVE_MODEL = 'model_folder'\n","LOG_MODEL = 'log_folder'\n","PATH_SUBMISSION = '/kaggle/input/nlp-getting-started/sample_submission.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T21:02:01.484227Z","iopub.status.busy":"2023-04-11T21:02:01.483830Z","iopub.status.idle":"2023-04-11T21:02:01.490716Z","shell.execute_reply":"2023-04-11T21:02:01.488464Z","shell.execute_reply.started":"2023-04-11T21:02:01.484192Z"},"trusted":true},"outputs":[],"source":["if not os.path.exists(SAVE_MODEL):\n","    os.makedirs(SAVE_MODEL)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T21:02:01.494279Z","iopub.status.busy":"2023-04-11T21:02:01.493390Z","iopub.status.idle":"2023-04-11T21:02:01.516269Z","shell.execute_reply":"2023-04-11T21:02:01.514967Z","shell.execute_reply.started":"2023-04-11T21:02:01.494238Z"},"trusted":true},"outputs":[],"source":["config = {\n","    'batch_size': 8,\n","    'num_workers': 1,\n","    #huggingface model\n","    'model': 'microsoft/deberta-v3-base',\n","    #entire script debug run\n","    'debug_run': False,\n","    #enable dev run on py light\n","    'dev_run': False,\n","    'n_fold': 5,\n","    'random_state': 1024157,\n","    'max_epochs': 6,\n","    #number of step. disable with -1.\n","    'max_steps': -1,\n","    #trainer parameter --> check loss every n step. put 0.95 to disable this.\n","    'val_check_interval': 0.1,\n","    'accelerator': \"gpu\" if torch.cuda.is_available() else \"cpu\",\n","    'lr': 1e-6,\n","    #save last epoch model\n","    'save_model': True,\n","    #used for logging\n","    'version_experiment': 'deberta_baseline_lr_1e5'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T21:02:01.519073Z","iopub.status.busy":"2023-04-11T21:02:01.518230Z","iopub.status.idle":"2023-04-11T21:02:01.579241Z","shell.execute_reply":"2023-04-11T21:02:01.578258Z","shell.execute_reply.started":"2023-04-11T21:02:01.519020Z"},"trusted":true},"outputs":[],"source":["logging.info('importing dataset')\n","\n","if config['debug_run']:\n","    train = pd.read_pickle(os.path.join(PATH_DATA, 'train.pkl')).sample(150).reset_index(drop=True)\n","    assert train['fold_cv'].nunique() == 5\n","else:\n","    train = pd.read_pickle(os.path.join(PATH_DATA, 'train.pkl'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:59:09.692647Z","iopub.status.busy":"2023-04-11T15:59:09.692182Z","iopub.status.idle":"2023-04-11T15:59:09.701806Z","shell.execute_reply":"2023-04-11T15:59:09.700718Z","shell.execute_reply.started":"2023-04-11T15:59:09.692611Z"},"trusted":true},"outputs":[],"source":["if config['save_model']:\n","    print(f\"Fitting model for exactly {config['max_epochs']} epoch and {config['max_steps']} step\")\n","    logging.info(f\"Fitting model for exactly {config['max_epochs']} epoch and {config['max_steps']} step\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:59:09.703996Z","iopub.status.busy":"2023-04-11T15:59:09.703415Z","iopub.status.idle":"2023-04-11T15:59:09.709791Z","shell.execute_reply":"2023-04-11T15:59:09.708670Z","shell.execute_reply.started":"2023-04-11T15:59:09.703956Z"},"trusted":true},"outputs":[],"source":["transformers.logging.set_verbosity_error()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:59:09.712448Z","iopub.status.busy":"2023-04-11T15:59:09.711273Z","iopub.status.idle":"2023-04-11T15:59:12.156164Z","shell.execute_reply":"2023-04-11T15:59:12.155031Z","shell.execute_reply.started":"2023-04-11T15:59:09.712366Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(config['model'])\n","\n","lengths = [\n","    len(tokenizer(text, add_special_tokens=True)['input_ids'])\n","    for text in train['text_keyword_cleaned'].fillna(\"\").values\n","]\n","config['max_len'] = max(lengths)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:59:12.157832Z","iopub.status.busy":"2023-04-11T15:59:12.157469Z","iopub.status.idle":"2023-04-11T15:59:12.165861Z","shell.execute_reply":"2023-04-11T15:59:12.164742Z","shell.execute_reply.started":"2023-04-11T15:59:12.157797Z"},"trusted":true},"outputs":[],"source":["def best_threshold(y_true, pred_proba, proba_range = np.arange(.1,.9,.001), verbose=False): \n","    scores = []\n","    for prob in proba_range:\n","        pred = [int(p>prob) for p in pred_proba]\n","        score = f1_score(y_true,pred)\n","        scores.append(score)\n","        if verbose:\n","            print(\"INFO: prob threshold: {}.  score :{}\".format(round(prob,3), round(score,5)))\n","    best_score = scores[np.argmax(scores)]\n","    optimal_threshold = proba_range[np.argmax(scores)]\n","    return (optimal_threshold, best_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:59:12.170324Z","iopub.status.busy":"2023-04-11T15:59:12.169372Z","iopub.status.idle":"2023-04-11T15:59:12.177582Z","shell.execute_reply":"2023-04-11T15:59:12.176745Z","shell.execute_reply.started":"2023-04-11T15:59:12.170286Z"},"trusted":true},"outputs":[],"source":["class MeanPooling(nn.Module):\n","    \n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:59:12.179720Z","iopub.status.busy":"2023-04-11T15:59:12.178733Z","iopub.status.idle":"2023-04-11T15:59:12.189670Z","shell.execute_reply":"2023-04-11T15:59:12.188893Z","shell.execute_reply.started":"2023-04-11T15:59:12.179668Z"},"trusted":true},"outputs":[],"source":["class TweetDataset(Dataset):\n","    def __init__(self, \n","                 config, dataset, inference, \n","                 text_col_name: str='text_keyword_cleaned', label_col_name: str = 'target_relabeled'\n","        ):\n","        self.max_len = config['max_len']\n","        self.tokenizer = AutoTokenizer.from_pretrained(config['model'])\n","        self.texts = dataset[text_col_name].values\n","        self.inference = inference\n","    \n","        if not inference:\n","            self.labels = dataset[label_col_name].values\n","\n","    def prepare_input(self, text):\n","        inputs = self.tokenizer(\n","            text,\n","            return_tensors=None, \n","            add_special_tokens=True, \n","            max_length=self.max_len,\n","            padding='max_length', truncation=True\n","        )\n","\n","        return {key: torch.tensor(value, dtype=torch.long) for key, value in inputs.items()}\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        inputs = self.prepare_input(self.texts[item])\n","        \n","        if self.inference:\n","            return inputs\n","        else:\n","            label = torch.tensor(self.labels[item], dtype=torch.float)\n","        \n","            return inputs, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:59:12.191775Z","iopub.status.busy":"2023-04-11T15:59:12.191105Z","iopub.status.idle":"2023-04-11T15:59:12.214213Z","shell.execute_reply":"2023-04-11T15:59:12.213236Z","shell.execute_reply.started":"2023-04-11T15:59:12.191735Z"},"trusted":true},"outputs":[],"source":["class DeBertaClassifier(pl.LightningModule):\n","    def __init__(self, config):\n","        super().__init__()\n","        \n","        self.model = AutoModel.from_pretrained(config['model'])\n","        \n","        self.criterion = nn.BCEWithLogitsLoss()\n","        \n","        self.config = config\n","        \n","        if self.config['lr'] is not None:\n","            self.lr = self.config['lr']\n","        \n","        self.fc = nn.Linear(self.model.config.hidden_size, 1)\n","        self.pool = MeanPooling()\n","        \n","        self.auc = BinaryAUROC(pos_label=1)\n","        self.f1 = BinaryF1Score()\n","        \n","        self.step_outputs = {\n","            'train': [],\n","            'val': [],\n","            'test': []\n","        }\n","        self.save_hyperparameters()\n","        \n","    def __metric_step(self, pred, labels):\n","        pred = torch.sigmoid(torch.flatten(pred))\n","        \n","        auc_score = self.auc(pred, labels)\n","        f1_score = self.f1(pred, labels)\n","        \n","        return {'auc': auc_score, 'f1': f1_score}\n","    \n","    def __loss_step(self, pred, labels):\n","\n","        pred = torch.flatten(pred)\n","        loss = self.criterion(pred, labels)\n","        \n","        return loss, pred, labels\n","    \n","    def training_step(self, batch, batch_idx):\n","\n","        input_, labels = batch\n","            \n","        pred = self.forward(input_)\n","        loss, _, _ = self.__loss_step(pred, labels)\n","        self.step_outputs['train'].append(\n","            {'loss': loss}\n","        )\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_, labels = batch\n","        pred = self.forward(input_)\n","        \n","        loss, pred, labels = self.__loss_step(pred, labels)\n","        self.step_outputs['val'].append(\n","            {'loss': loss, 'pred': pred, 'labels': labels}\n","        )\n","        \n","    def test_step(self, batch, batch_idx):\n","        input_, labels = batch\n","        pred = self.forward(input_)\n","        \n","        loss, pred, labels = self.__loss_step(pred, labels)\n","        self.step_outputs['test'].append(\n","            {'loss': loss, 'pred': pred, 'labels': labels}\n","        )\n","\n","    def on_training_epoch_end(self):\n","        self.__share_eval_res('train')\n","        \n","    def on_validation_epoch_end(self):\n","        self.__share_eval_res('val')\n","\n","    def on_test_epoch_end(self):\n","        self.__share_eval_res('test')\n","    \n","    def __log_loss_step(self, mode):\n","        loss = [out['loss'].reshape(1) for out in outputs]\n","        loss = torch.mean(torch.cat(loss))\n","        \n","        #initialize performance output\n","        res_dict = {\n","            f'{mode}_loss': loss\n","        }\n","\n","    def __share_eval_res(self, mode: str):\n","        outputs = self.step_outputs[mode]\n","        loss = [out['loss'].reshape(1) for out in outputs]\n","        loss = torch.mean(torch.cat(loss))\n","        \n","        #initialize performance output\n","        res_dict = {\n","            f'{mode}_loss': loss\n","        }\n","        metric_message_list = [\n","            f'step: {self.trainer.global_step}',\n","            f'{mode}_loss: {loss:.5f}'\n","        ]\n","        #evaluate on all dataset\n","        if mode != 'train':\n","            preds = [out['pred'] for out in outputs]\n","            preds = torch.cat(preds)\n","            \n","            labels = [out['labels'] for out in outputs]\n","            labels = torch.cat(labels)\n","        \n","            metric_score = self.__metric_step(preds, labels)\n","            \n","            #calculate every metric on all batch\n","            metric_message_list += [\n","                f'{mode}_{metric}: {metric_value:.5f}'\n","                for metric, metric_value in metric_score.items()\n","            ]\n","            #get results\n","            res_dict.update(\n","                {\n","                    f'{mode}_{metric}': metric_value\n","                    for metric, metric_value in metric_score.items()\n","                }\n","            )\n","        else:\n","            pass\n","\n","        if self.trainer.sanity_checking:\n","            pass\n","        else:\n","            print(', '.join(metric_message_list))\n","            self.log_dict(res_dict)\n","            \n","        #free memory\n","        self.step_outputs[mode].clear()\n","    \n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","        return optimizer\n","    \n","    def __encoder(self, inputs):\n","\n","        outputs = self.model(**inputs)\n","\n","        last_hidden_states = outputs['last_hidden_state']\n","        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        \n","        return feature\n","\n","    def forward(self, inputs):\n","        \n","        feature = self.__encoder(inputs)\n","        \n","        output = self.fc(feature)\n","        \n","        return output\n","    \n","    def predict_step(self, batch, batch_idx):\n","        pred = self.forward(batch)\n","        pred = torch.flatten(pred)\n","        pred = torch.sigmoid(pred)\n","        \n","        return pred"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:59:12.217318Z","iopub.status.busy":"2023-04-11T15:59:12.216980Z","iopub.status.idle":"2023-04-11T15:59:12.230324Z","shell.execute_reply":"2023-04-11T15:59:12.229087Z","shell.execute_reply.started":"2023-04-11T15:59:12.217282Z"},"trusted":true},"outputs":[],"source":["def train_folder(\n","    fold_: int, train_data: pd.DataFrame, valid_data: pd.DataFrame,\n","    config: dict = config, save_model: str = SAVE_MODEL, \n","    log_model: str = LOG_MODEL\n",") -> None:\n","    model_folder = os.path.join(save_model, f'model_fold_{fold_}')\n","    log_folder = os.path.join(log_model, f'log_fold_{fold_}')\n","    \n","    if not os.path.exists(model_folder):\n","        os.makedirs(model_folder)\n","\n","    if not os.path.exists(log_folder):\n","        os.makedirs(log_folder)\n","    \n","    train_dataset = TweetDataset(config, train_data, inference=False)\n","    valid_dataset = TweetDataset(config, valid_data, inference=False)\n","\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=config['batch_size'],\n","        shuffle=True,\n","        pin_memory=True,\n","        drop_last=True\n","    )\n","    \n","    valid_loader = DataLoader(\n","        valid_dataset,\n","        batch_size=config['batch_size']*2,\n","        shuffle=False,\n","        drop_last=False\n","    )\n","    \n","    model_ = DeBertaClassifier(config)\n","    loggers = pl.loggers.CSVLogger(\n","        save_dir=log_folder,\n","        name='csv_log',\n","        version=config['version_experiment']\n","    )\n","\n","    trainer = pl.Trainer(\n","        max_epochs=config['max_epochs'],\n","        max_steps=config['max_steps'],\n","        fast_dev_run=config['dev_run'], \n","        accelerator=config['accelerator'],\n","        val_check_interval=config['val_check_interval'],\n","        enable_progress_bar=False,\n","        logger=[loggers],\n","        enable_checkpointing=False\n","    )\n","    \n","    trainer.fit(model_, train_loader, valid_loader)\n","    \n","    if config['save_model']:\n","        trainer.save_checkpoint(os.path.join(model_folder, \"model.ckpt\"))\n","        \n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:59:12.232501Z","iopub.status.busy":"2023-04-11T15:59:12.232062Z","iopub.status.idle":"2023-04-11T16:01:18.098279Z","shell.execute_reply":"2023-04-11T16:01:18.097154Z","shell.execute_reply.started":"2023-04-11T15:59:12.232466Z"},"trusted":true},"outputs":[],"source":["#score cv to get best cv epoch and after select num_epoch and retrain\n","for fold_ in range(config['n_fold']):\n","    train_data = train.loc[train['fold_cv']!=fold_].reset_index(drop=True)\n","    valid_data = train.loc[train['fold_cv']==fold_].reset_index(drop=True)\n","\n","    print(f'\\n\\nStarting folder {fold_}\\n\\n')\n","    train_folder(fold_, train_data, valid_data)\n","    \n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Log metric"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T16:06:55.543146Z","iopub.status.busy":"2023-04-11T16:06:55.542069Z","iopub.status.idle":"2023-04-11T16:06:55.562164Z","shell.execute_reply":"2023-04-11T16:06:55.561114Z","shell.execute_reply.started":"2023-04-11T16:06:55.543107Z"},"trusted":true},"outputs":[],"source":["log_file_path = sorted(glob(LOG_MODEL+ f\"/log_fold_*/csv_log/{config['version_experiment']}/*.csv\"))\n","assert len(log_file_path) == config['n_fold']\n","log_list = pd.concat(\n","    [\n","        (pd.read_csv(file_path)).assign(fold=fold_)\n","        for fold_, file_path in enumerate(log_file_path)\n","    ], axis=0,ignore_index=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T16:07:11.962254Z","iopub.status.busy":"2023-04-11T16:07:11.961855Z","iopub.status.idle":"2023-04-11T16:07:11.974068Z","shell.execute_reply":"2023-04-11T16:07:11.972895Z","shell.execute_reply.started":"2023-04-11T16:07:11.962218Z"},"trusted":true},"outputs":[],"source":["log_list = log_list.groupby(\n","    ['step', 'epoch']\n",").agg(\n","    {\n","        col: 'mean'\n","        for col in log_list.columns if 'val_' in col\n","    }\n",").reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T16:02:54.099018Z","iopub.status.busy":"2023-04-11T16:02:54.098625Z","iopub.status.idle":"2023-04-11T16:02:54.171768Z","shell.execute_reply":"2023-04-11T16:02:54.170574Z","shell.execute_reply.started":"2023-04-11T16:02:54.098985Z"},"trusted":true},"outputs":[],"source":["fig = px.line(log_list, x='step', y=[col for col in log_list if 'val_' in col], template='plotly_white')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T16:10:40.254742Z","iopub.status.busy":"2023-04-11T16:10:40.254364Z","iopub.status.idle":"2023-04-11T16:10:40.263785Z","shell.execute_reply":"2023-04-11T16:10:40.262696Z","shell.execute_reply.started":"2023-04-11T16:10:40.254710Z"},"trusted":true},"outputs":[],"source":["best_pos = int(log_list['val_loss'].argmin())\n","\n","best_epoch = log_list.loc[best_pos, 'epoch']\n","best_step = log_list.loc[best_pos, 'step']\n","best_score = log_list['val_loss'].min()\n","\n","print(f'Best epoch: {best_epoch} Best step: {best_step}, CV-Loss: {best_score:.5f}\\n')\n","print('Other information\\n')\n","print(log_list.loc[best_pos])\n","\n","best_result = {\n","    'best_epoch': best_epoch,\n","    'best_step': best_step,\n","    'best_score': best_score\n","}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Predict valid and test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test = pd.read_pickle(os.path.join(PATH_DATA, 'test.pkl'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#score cv to get best cv epoch and after select num_epoch and retrain\n","predictions_valid = np.zeros((train.shape[0]))\n","prediction_test = np.zeros((test.shape[0]))\n","\n","trainer = pl.Trainer(accelerator=config['accelerator'])\n","\n","test_dataset = TweetDataset(config, test, inference=True)\n","\n","for fold_ in range(config['n_fold']):\n","    #load the model\n","    model_folder = os.path.join(SAVE_MODEL, f'model_fold_{fold_}', 'model.ckpt')\n","    model = DeBertaClassifier(config).load_from_checkpoint(model_folder)\n","    model.eval()\n","\n","    #import oof data and test to predict\n","    mask_valid = train['fold_cv']==fold_\n","    valid_data = train.loc[mask_valid].reset_index(drop=True)\n","    \n","    valid_dataset = TweetDataset(config, valid_data, inference=True)\n","    valid_loader = DataLoader(\n","        valid_dataset,\n","        batch_size=config['batch_size']*2,\n","        shuffle=False,\n","        drop_last=False\n","    )\n","        \n","    #predict valid\n","    pred_val = trainer.predict(model, valid_loader)\n","    pred_val = torch.concat(pred_val).numpy().reshape((-1))\n","    \n","    predictions_valid[mask_valid] = pred_val\n","    \n","    #predict test by resetting data loader\n","    test_loader = DataLoader(\n","        test_dataset,\n","        batch_size=config['batch_size']*2,\n","        shuffle=False,\n","        drop_last=False\n","    )\n","\n","    pred_test = trainer.predict(model, test_loader)\n","    pred_test = torch.concat(pred_test).numpy().reshape((-1))\n","    \n","    prediction_test += pred_test/config['n_fold']\n","\n","    torch.cuda.empty_cache()\n","    _ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["optimal_threshold, best_score = best_threshold(train['target_relabeled'].values, predictions_valid)\n","\n","print(f'Optimal Threshold is {optimal_threshold:.3f}; with the F1 Score of {best_score:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del train\n","_ = gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prediction_test = (prediction_test >= optimal_threshold).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub = pd.read_csv(PATH_SUBMISSION)\n","assert sub.shape[0] == prediction_test.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub['target'] = prediction_test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
