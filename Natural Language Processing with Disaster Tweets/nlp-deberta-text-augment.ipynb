{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers sentencepiece","metadata":{"execution":{"iopub.status.busy":"2023-04-11T21:01:37.719802Z","iopub.execute_input":"2023-04-11T21:01:37.720227Z","iopub.status.idle":"2023-04-11T21:01:51.442437Z","shell.execute_reply.started":"2023-04-11T21:01:37.720189Z","shell.execute_reply":"2023-04-11T21:01:51.441111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport logging\nimport gc\nimport transformers\n\nimport pandas as pd\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nimport plotly.express as px\n\nfrom transformers import AutoModel, AutoTokenizer\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom glob import glob\n\nfrom torchmetrics.classification import BinaryAUROC, BinaryF1Score\n\nPATH_DATA = '/kaggle/input/nlp-get-started-cleaning-text-tr'\n\nSAVE_MODEL = 'model_folder'\nLOG_MODEL = 'log_folder'\nPATH_SUBMISSION = '/kaggle/input/nlp-getting-started/sample_submission.csv'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-11T21:01:51.445540Z","iopub.execute_input":"2023-04-11T21:01:51.445910Z","iopub.status.idle":"2023-04-11T21:02:01.482278Z","shell.execute_reply.started":"2023-04-11T21:01:51.445854Z","shell.execute_reply":"2023-04-11T21:02:01.480828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(SAVE_MODEL):\n    os.makedirs(SAVE_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T21:02:01.483830Z","iopub.execute_input":"2023-04-11T21:02:01.484227Z","iopub.status.idle":"2023-04-11T21:02:01.490716Z","shell.execute_reply.started":"2023-04-11T21:02:01.484192Z","shell.execute_reply":"2023-04-11T21:02:01.488464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'batch_size': 8,\n    'num_workers': 1,\n    #huggingface model\n    'model': 'microsoft/deberta-v3-base',\n    #entire script debug run\n    'debug_run': False,\n    #enable dev run on py light\n    'dev_run': False,\n    'n_fold': 5,\n    'random_state': 1024157,\n    'max_epochs': 6,\n    #number of step. disable with -1.\n    'max_steps': -1,\n    #trainer parameter --> check loss every n step. put 0.95 to disable this.\n    'val_check_interval': 0.1,\n    'accelerator': \"gpu\" if torch.cuda.is_available() else \"cpu\",\n    'lr': 1e-6,\n    #save last epoch model\n    'save_model': True,\n    #used for logging\n    'version_experiment': 'deberta_baseline_lr_1e5'\n}","metadata":{"execution":{"iopub.status.busy":"2023-04-11T21:02:01.493390Z","iopub.execute_input":"2023-04-11T21:02:01.494279Z","iopub.status.idle":"2023-04-11T21:02:01.516269Z","shell.execute_reply.started":"2023-04-11T21:02:01.494238Z","shell.execute_reply":"2023-04-11T21:02:01.514967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.info('importing dataset')\n\nif config['debug_run']:\n    train = pd.read_pickle(os.path.join(PATH_DATA, 'train.pkl')).sample(150).reset_index(drop=True)\n    assert train['fold_cv'].nunique() == 5\nelse:\n    train = pd.read_pickle(os.path.join(PATH_DATA, 'train.pkl'))","metadata":{"execution":{"iopub.status.busy":"2023-04-11T21:02:01.518230Z","iopub.execute_input":"2023-04-11T21:02:01.519073Z","iopub.status.idle":"2023-04-11T21:02:01.579241Z","shell.execute_reply.started":"2023-04-11T21:02:01.519020Z","shell.execute_reply":"2023-04-11T21:02:01.578258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config['save_model']:\n    print(f\"Fitting model for exactly {config['max_epochs']} epoch and {config['max_steps']} step\")\n    logging.info(f\"Fitting model for exactly {config['max_epochs']} epoch and {config['max_steps']} step\")","metadata":{"execution":{"iopub.status.busy":"2023-04-11T15:59:09.692182Z","iopub.execute_input":"2023-04-11T15:59:09.692647Z","iopub.status.idle":"2023-04-11T15:59:09.701806Z","shell.execute_reply.started":"2023-04-11T15:59:09.692611Z","shell.execute_reply":"2023-04-11T15:59:09.700718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformers.logging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2023-04-11T15:59:09.703415Z","iopub.execute_input":"2023-04-11T15:59:09.703996Z","iopub.status.idle":"2023-04-11T15:59:09.709791Z","shell.execute_reply.started":"2023-04-11T15:59:09.703956Z","shell.execute_reply":"2023-04-11T15:59:09.708670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config['model'])\n\nlengths = [\n    len(tokenizer(text, add_special_tokens=True)['input_ids'])\n    for text in train['text_keyword_cleaned'].fillna(\"\").values\n]\nconfig['max_len'] = max(lengths)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T15:59:09.711273Z","iopub.execute_input":"2023-04-11T15:59:09.712448Z","iopub.status.idle":"2023-04-11T15:59:12.156164Z","shell.execute_reply.started":"2023-04-11T15:59:09.712366Z","shell.execute_reply":"2023-04-11T15:59:12.155031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def best_threshold(y_true, pred_proba, proba_range = np.arange(.1,.9,.001), verbose=False): \n    scores = []\n    for prob in proba_range:\n        pred = [int(p>prob) for p in pred_proba]\n        score = f1_score(y_true,pred)\n        scores.append(score)\n        if verbose:\n            print(\"INFO: prob threshold: {}.  score :{}\".format(round(prob,3), round(score,5)))\n    best_score = scores[np.argmax(scores)]\n    optimal_threshold = proba_range[np.argmax(scores)]\n    return (optimal_threshold, best_score)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T15:59:12.157469Z","iopub.execute_input":"2023-04-11T15:59:12.157832Z","iopub.status.idle":"2023-04-11T15:59:12.165861Z","shell.execute_reply.started":"2023-04-11T15:59:12.157797Z","shell.execute_reply":"2023-04-11T15:59:12.164742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    \n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings","metadata":{"execution":{"iopub.status.busy":"2023-04-11T15:59:12.169372Z","iopub.execute_input":"2023-04-11T15:59:12.170324Z","iopub.status.idle":"2023-04-11T15:59:12.177582Z","shell.execute_reply.started":"2023-04-11T15:59:12.170286Z","shell.execute_reply":"2023-04-11T15:59:12.176745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TweetDataset(Dataset):\n    def __init__(self, \n                 config, dataset, inference, \n                 text_col_name: str='text_keyword_cleaned', label_col_name: str = 'target_relabeled'\n        ):\n        self.max_len = config['max_len']\n        self.tokenizer = AutoTokenizer.from_pretrained(config['model'])\n        self.texts = dataset[text_col_name].values\n        self.inference = inference\n    \n        if not inference:\n            self.labels = dataset[label_col_name].values\n\n    def prepare_input(self, text):\n        inputs = self.tokenizer(\n            text,\n            return_tensors=None, \n            add_special_tokens=True, \n            max_length=self.max_len,\n            padding='max_length', truncation=True\n        )\n\n        return {key: torch.tensor(value, dtype=torch.long) for key, value in inputs.items()}\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = self.prepare_input(self.texts[item])\n        \n        if self.inference:\n            return inputs\n        else:\n            label = torch.tensor(self.labels[item], dtype=torch.float)\n        \n            return inputs, label","metadata":{"execution":{"iopub.status.busy":"2023-04-11T15:59:12.178733Z","iopub.execute_input":"2023-04-11T15:59:12.179720Z","iopub.status.idle":"2023-04-11T15:59:12.189670Z","shell.execute_reply.started":"2023-04-11T15:59:12.179668Z","shell.execute_reply":"2023-04-11T15:59:12.188893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DeBertaClassifier(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.model = AutoModel.from_pretrained(config['model'])\n        \n        self.criterion = nn.BCEWithLogitsLoss()\n        \n        self.config = config\n        \n        if self.config['lr'] is not None:\n            self.lr = self.config['lr']\n        \n        self.fc = nn.Linear(self.model.config.hidden_size, 1)\n        self.pool = MeanPooling()\n        \n        self.auc = BinaryAUROC(pos_label=1)\n        self.f1 = BinaryF1Score()\n        \n        self.step_outputs = {\n            'train': [],\n            'val': [],\n            'test': []\n        }\n        self.save_hyperparameters()\n        \n    def __metric_step(self, pred, labels):\n        pred = torch.flatten(pred)\n        \n        auc_score = self.auc(pred, labels)\n        f1_score = self.f1(pred, labels)\n        \n        return {'auc': auc_score, 'f1': f1_score}\n    \n    def __loss_step(self, pred, labels):\n\n        pred = torch.flatten(pred)\n        loss = self.criterion(pred, labels)\n        \n        return loss, pred, labels\n    \n    def training_step(self, batch, batch_idx):\n\n        input_, labels = batch\n            \n        pred = self.forward(input_)\n        loss, _, _ = self.__loss_step(pred, labels)\n        self.step_outputs['train'].append(\n            {'loss': loss}\n        )\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        input_, labels = batch\n        pred = self.forward(input_)\n        \n        loss, pred, labels = self.__loss_step(pred, labels)\n        self.step_outputs['val'].append(\n            {'loss': loss, 'pred': pred, 'labels': labels}\n        )\n        \n    def test_step(self, batch, batch_idx):\n        input_, labels = batch\n        pred = self.forward(input_)\n        \n        loss, pred, labels = self.__loss_step(pred, labels)\n        self.step_outputs['test'].append(\n            {'loss': loss, 'pred': pred, 'labels': labels}\n        )\n\n    def on_training_epoch_end(self):\n        self.__share_eval_res('train')\n        \n    def on_validation_epoch_end(self):\n        self.__share_eval_res('val')\n\n    def on_test_epoch_end(self):\n        self.__share_eval_res('test')\n    \n    def __log_loss_step(self, mode):\n        loss = [out['loss'].reshape(1) for out in outputs]\n        loss = torch.mean(torch.cat(loss))\n        \n        #initialize performance output\n        res_dict = {\n            f'{mode}_loss': loss\n        }\n\n    def __share_eval_res(self, mode: str):\n        outputs = self.step_outputs[mode]\n        loss = [out['loss'].reshape(1) for out in outputs]\n        loss = torch.mean(torch.cat(loss))\n        \n        #initialize performance output\n        res_dict = {\n            f'{mode}_loss': loss\n        }\n        metric_message_list = [\n            f'step: {self.trainer.global_step}',\n            f'{mode}_loss: {loss:.5f}'\n        ]\n        #evaluate on all dataset\n        if mode != 'train':\n            preds = [out['pred'] for out in outputs]\n            preds = torch.cat(preds)\n            \n            labels = [out['labels'] for out in outputs]\n            labels = torch.cat(labels)\n        \n            metric_score = self.__metric_step(preds, labels)\n            \n            #calculate every metric on all batch\n            metric_message_list += [\n                f'{mode}_{metric}: {metric_value:.5f}'\n                for metric, metric_value in metric_score.items()\n            ]\n            #get results\n            res_dict.update(\n                {\n                    f'{mode}_{metric}': metric_value\n                    for metric, metric_value in metric_score.items()\n                }\n            )\n        else:\n            pass\n\n        if self.trainer.sanity_checking:\n            pass\n        else:\n            print(', '.join(metric_message_list))\n            self.log_dict(res_dict)\n            \n        #free memory\n        self.step_outputs[mode].clear()\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n    \n    def __encoder(self, inputs):\n\n        outputs = self.model(**inputs)\n\n        last_hidden_states = outputs['last_hidden_state']\n        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        \n        return feature\n\n    def forward(self, inputs):\n        \n        feature = self.__encoder(inputs)\n        \n        output = self.fc(feature)\n        \n        return output\n    \n    def predict_step(self, batch, batch_idx):\n        pred = self.forward(batch)\n        pred = torch.flatten(pred)\n        pred = torch.sigmoid(pred)\n        \n        return pred","metadata":{"execution":{"iopub.status.busy":"2023-04-11T15:59:12.191105Z","iopub.execute_input":"2023-04-11T15:59:12.191775Z","iopub.status.idle":"2023-04-11T15:59:12.214213Z","shell.execute_reply.started":"2023-04-11T15:59:12.191735Z","shell.execute_reply":"2023-04-11T15:59:12.213236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_folder(\n    fold_: int, train_data: pd.DataFrame, valid_data: pd.DataFrame,\n    config: dict = config, save_model: str = SAVE_MODEL, \n    log_model: str = LOG_MODEL\n) -> None:\n    model_folder = os.path.join(save_model, f'model_fold_{fold_}')\n    log_folder = os.path.join(log_model, f'log_fold_{fold_}')\n    \n    if not os.path.exists(model_folder):\n        os.makedirs(model_folder)\n\n    if not os.path.exists(log_folder):\n        os.makedirs(log_folder)\n    \n    train_dataset = TweetDataset(config, train_data, inference=False)\n    valid_dataset = TweetDataset(config, valid_data, inference=False)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        pin_memory=True,\n        drop_last=True\n    )\n    \n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=config['batch_size']*2,\n        shuffle=False,\n        drop_last=False\n    )\n    \n    model_ = DeBertaClassifier(config)\n    loggers = pl.loggers.CSVLogger(\n        save_dir=log_folder,\n        name='csv_log',\n        version=config['version_experiment']\n    )\n\n    trainer = pl.Trainer(\n        max_epochs=config['max_epochs'],\n        max_steps=config['max_steps'],\n        fast_dev_run=config['dev_run'], \n        accelerator=config['accelerator'],\n        val_check_interval=config['val_check_interval'],\n        enable_progress_bar=False,\n        logger=[loggers],\n        enable_checkpointing=False\n    )\n    \n    trainer.fit(model_, train_loader, valid_loader)\n    \n    if config['save_model']:\n        trainer.save_checkpoint(os.path.join(model_folder, \"model.ckpt\"))\n        \n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-11T15:59:12.216980Z","iopub.execute_input":"2023-04-11T15:59:12.217318Z","iopub.status.idle":"2023-04-11T15:59:12.230324Z","shell.execute_reply.started":"2023-04-11T15:59:12.217282Z","shell.execute_reply":"2023-04-11T15:59:12.229087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score cv to get best cv epoch and after select num_epoch and retrain\nfor fold_ in range(config['n_fold']):\n    train_data = train.loc[train['fold_cv']!=fold_].reset_index(drop=True)\n    valid_data = train.loc[train['fold_cv']==fold_].reset_index(drop=True)\n\n    print(f'\\n\\nStarting folder {fold_}\\n\\n')\n    train_folder(fold_, train_data, valid_data)\n    \n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-11T15:59:12.232062Z","iopub.execute_input":"2023-04-11T15:59:12.232501Z","iopub.status.idle":"2023-04-11T16:01:18.098279Z","shell.execute_reply.started":"2023-04-11T15:59:12.232466Z","shell.execute_reply":"2023-04-11T16:01:18.097154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Log metric","metadata":{}},{"cell_type":"code","source":"log_file_path = sorted(glob(LOG_MODEL+ f\"/log_fold_*/csv_log/{config['version_experiment']}/*.csv\"))\nassert len(log_file_path) == config['n_fold']\nlog_list = pd.concat(\n    [\n        (pd.read_csv(file_path)).assign(fold=fold_)\n        for fold_, file_path in enumerate(log_file_path)\n    ], axis=0,ignore_index=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T16:06:55.542069Z","iopub.execute_input":"2023-04-11T16:06:55.543146Z","iopub.status.idle":"2023-04-11T16:06:55.562164Z","shell.execute_reply.started":"2023-04-11T16:06:55.543107Z","shell.execute_reply":"2023-04-11T16:06:55.561114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_list = log_list.groupby(\n    ['step', 'epoch']\n).agg(\n    {\n        col: 'mean'\n        for col in log_list.columns if 'val_' in col\n    }\n).reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-04-11T16:07:11.961855Z","iopub.execute_input":"2023-04-11T16:07:11.962254Z","iopub.status.idle":"2023-04-11T16:07:11.974068Z","shell.execute_reply.started":"2023-04-11T16:07:11.962218Z","shell.execute_reply":"2023-04-11T16:07:11.972895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.line(log_list, x='step', y=[col for col in log_list if 'val_' in col], template='plotly_white')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-11T16:02:54.098625Z","iopub.execute_input":"2023-04-11T16:02:54.099018Z","iopub.status.idle":"2023-04-11T16:02:54.171768Z","shell.execute_reply.started":"2023-04-11T16:02:54.098985Z","shell.execute_reply":"2023-04-11T16:02:54.170574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_pos = int(log_list['val_loss'].argmin())\n\nbest_epoch = log_list.loc[best_pos, 'epoch']\nbest_step = log_list.loc[best_pos, 'step']\nbest_score = log_list['val_loss'].min()\n\nprint(f'Best epoch: {best_epoch} Best step: {best_step}, CV-Loss: {best_score:.5f}\\n')\nprint('Other information\\n')\nprint(log_list.loc[best_pos])\n\nbest_result = {\n    'best_epoch': best_epoch,\n    'best_step': best_step,\n    'best_score': best_score\n}","metadata":{"execution":{"iopub.status.busy":"2023-04-11T16:10:40.254364Z","iopub.execute_input":"2023-04-11T16:10:40.254742Z","iopub.status.idle":"2023-04-11T16:10:40.263785Z","shell.execute_reply.started":"2023-04-11T16:10:40.254710Z","shell.execute_reply":"2023-04-11T16:10:40.262696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict valid and test","metadata":{}},{"cell_type":"code","source":"test = pd.read_pickle(os.path.join(PATH_DATA, 'test.pkl'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score cv to get best cv epoch and after select num_epoch and retrain\npredictions_valid = np.zeros((train.shape[0]))\nprediction_test = np.zeros((test.shape[0]))\n\ntrainer = pl.Trainer(accelerator=config['accelerator'])\n\ntest_dataset = TweetDataset(config, test, inference=True)\n\nfor fold_ in range(config['n_fold']):\n    #load the model\n    model_folder = os.path.join(SAVE_MODEL, f'model_fold_{fold_}', 'model.ckpt')\n    model = DeBertaClassifier(config).load_from_checkpoint(model_folder)\n    model.eval()\n\n    #import oof data and test to predict\n    mask_valid = train['fold_cv']==fold_\n    valid_data = train.loc[mask_valid].reset_index(drop=True)\n    \n    valid_dataset = TweetDataset(config, valid_data, inference=True)\n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=config['batch_size']*2,\n        shuffle=False,\n        drop_last=False\n    )\n        \n    #predict valid\n    pred_val = trainer.predict(model, valid_loader)\n    pred_val = torch.concat(pred_val).numpy().reshape((-1))\n    \n    predictions_valid[mask_valid] = pred_val\n    \n    #predict test by resetting data loader\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['batch_size']*2,\n        shuffle=False,\n        drop_last=False\n    )\n\n    pred_test = trainer.predict(model, test_loader)\n    pred_test = torch.concat(pred_test).numpy().reshape((-1))\n    \n    prediction_test += pred_test/config['n_fold']\n\n    torch.cuda.empty_cache()\n    _ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimal_threshold, best_score = best_threshold(train['target_relabeled'].values, predictions_valid)\n\nprint(f'Optimal Threshold is {optimal_threshold:.3f}; with the F1 Score of {best_score:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"prediction_test = (prediction_test >= optimal_threshold).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(PATH_SUBMISSION)\nassert sub.shape[0] == prediction_test.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] = prediction_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}