{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.3\r\n",
      "  Downloading pandas-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 2.6 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2.8.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (1.18.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.3) (1.14.0)\r\n",
      "\u001b[31mERROR: hypertools 0.6.2 has requirement scikit-learn<0.22,>=0.19.1, but you'll have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: datalab 1.1.5 has requirement pandas-profiling==1.4.0, but you'll have pandas-profiling 2.4.0 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: pandas\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 1.0.1\r\n",
      "    Uninstalling pandas-1.0.1:\r\n",
      "      Successfully uninstalled pandas-1.0.1\r\n",
      "Successfully installed pandas-1.0.3\r\n",
      "Collecting tensorflow==2.2.0-rc2\r\n",
      "  Downloading tensorflow-2.2.0rc2-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 1.7 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (1.18.1)\r\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0rc0\r\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 454 kB 58.1 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (1.1.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (3.2.1)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (0.9.0)\r\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (1.4.1)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (1.11.2)\r\n",
      "Collecting gast==0.3.3\r\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (0.2.0)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (3.11.3)\r\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (0.34.2)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (1.1.0)\r\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (2.10.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (1.14.0)\r\n",
      "Collecting astunparse==1.6.3\r\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\r\n",
      "  Downloading tensorboard-2.2.1-py3-none-any.whl (3.0 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 46.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2.0-rc2) (1.28.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow==2.2.0-rc2) (45.2.0.post20200209)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (1.11.2)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (1.0.0)\r\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 777 kB 34.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (3.2.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (2.23.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (0.4.1)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (4.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (3.1.1)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (2.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (1.25.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (2020.4.5.1)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (1.2.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (3.0.1)\r\n",
      "Installing collected packages: tensorflow-estimator, gast, astunparse, tensorboard-plugin-wit, tensorboard, tensorflow\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\r\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\r\n",
      "  Attempting uninstall: gast\r\n",
      "    Found existing installation: gast 0.2.2\r\n",
      "    Uninstalling gast-0.2.2:\r\n",
      "      Successfully uninstalled gast-0.2.2\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.1.1\r\n",
      "    Uninstalling tensorboard-2.1.1:\r\n",
      "      Successfully uninstalled tensorboard-2.1.1\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.1.0\r\n",
      "    Uninstalling tensorflow-2.1.0:\r\n",
      "      Successfully uninstalled tensorflow-2.1.0\r\n",
      "Successfully installed astunparse-1.6.3 gast-0.3.3 tensorboard-2.2.1 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0rc2 tensorflow-estimator-2.2.0\r\n",
      "Collecting tensorflow-addons\r\n",
      "  Downloading tensorflow_addons-0.10.0-cp37-cp37m-manylinux2010_x86_64.whl (1.0 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 3.5 kB/s \r\n",
      "\u001b[?25hCollecting typeguard>=2.7\r\n",
      "  Downloading typeguard-2.7.1-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: typeguard, tensorflow-addons\r\n",
      "Successfully installed tensorflow-addons-0.10.0 typeguard-2.7.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.0.3\n",
    "!pip install tensorflow==2.2.0-rc2\n",
    "!pip install tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from tensorflow.python.keras.engine import training\n",
    "import collections\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "LEN_ = 4000\n",
    "n_epoch = 350\n",
    "batch_size = 64\n",
    "folds = 5\n",
    "learning_rate = 0.001\n",
    "SEED_ = 987654321\n",
    "n_classes = 11\n",
    "\n",
    "lr_schedule = LearningRateScheduler(\n",
    "    PolynomialDecay(\n",
    "      initial_learning_rate = learning_rate,\n",
    "      decay_steps = n_epoch,\n",
    "      power = 0.5,\n",
    "      end_learning_rate = 0.0\n",
    "    )\n",
    ")\n",
    "class FBetaScore(tf.keras.metrics.Metric):\n",
    "    \"\"\"Computes F-Beta score.\n",
    "    It is the weighted harmonic mean of precision\n",
    "    and recall. Output range is [0, 1]. Works for\n",
    "    both multi-class and multi-label classification.\n",
    "    F-Beta = (1 + beta^2) * (prec * recall) / ((beta^2 * prec) + recall)\n",
    "    Args:\n",
    "        num_classes: Number of unique classes in the dataset.\n",
    "        average: Type of averaging to be performed on data.\n",
    "            Acceptable values are `None`, `micro`, `macro` and\n",
    "            `weighted`. Default value is None.\n",
    "        beta: Determines the weight of precision and recall\n",
    "            in harmonic mean. Determines the weight given to the\n",
    "            precision and recall. Default value is 1.\n",
    "        threshold: Elements of `y_pred` greater than threshold are\n",
    "            converted to be 1, and the rest 0. If threshold is\n",
    "            None, the argmax is converted to 1, and the rest 0.\n",
    "    Returns:\n",
    "        F-Beta Score: float\n",
    "    Raises:\n",
    "        ValueError: If the `average` has values other than\n",
    "        [None, micro, macro, weighted].\n",
    "        ValueError: If the `beta` value is less than or equal\n",
    "        to 0.\n",
    "    `average` parameter behavior:\n",
    "        None: Scores for each class are returned\n",
    "        micro: True positivies, false positives and\n",
    "            false negatives are computed globally.\n",
    "        macro: True positivies, false positives and\n",
    "            false negatives are computed for each class\n",
    "            and their unweighted mean is returned.\n",
    "        weighted: Metrics are computed for each class\n",
    "            and returns the mean weighted by the\n",
    "            number of true instances in each class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 shape = [LEN_, n_classes],\n",
    "                 average=None,\n",
    "                 beta=1.0,\n",
    "                 threshold=None,\n",
    "                 name='fbeta_score',\n",
    "                 dtype=tf.float32):\n",
    "        super(FBetaScore, self).__init__(name=name)\n",
    "\n",
    "        if average not in (None, 'micro', 'macro', 'weighted'):\n",
    "            raise ValueError(\"Unknown average type. Acceptable values \"\n",
    "                             \"are: [None, micro, macro, weighted]\")\n",
    "\n",
    "        if not isinstance(beta, float):\n",
    "            raise TypeError(\"The value of beta should be a python float\")\n",
    "\n",
    "        if beta <= 0.0:\n",
    "            raise ValueError(\"beta value should be greater than zero\")\n",
    "\n",
    "        if threshold is not None:\n",
    "            if not isinstance(threshold, float):\n",
    "                raise TypeError(\n",
    "                    \"The value of threshold should be a python float\")\n",
    "            if threshold > 1.0 or threshold <= 0.0:\n",
    "                raise ValueError(\"threshold should be between 0 and 1\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.average = average\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "        self.axis = None\n",
    "        self.init_shape = shape\n",
    "        \n",
    "        if self.average != 'micro':\n",
    "            self.axis = 0\n",
    "            # self.init_shape = [shape]\n",
    "\n",
    "        def _zero_wt_init(name):\n",
    "            return self.add_weight(\n",
    "                name,\n",
    "                shape=self.init_shape,\n",
    "                initializer='zeros',\n",
    "                dtype=self.dtype)\n",
    "\n",
    "        self.true_positives = _zero_wt_init('true_positives')\n",
    "        self.false_positives = _zero_wt_init('false_positives')\n",
    "        self.false_negatives = _zero_wt_init('false_negatives')\n",
    "        self.weights_intermediate = _zero_wt_init('weights_intermediate')\n",
    "\n",
    "    # TODO: Add sample_weight support, currently it is\n",
    "    # ignored during calculations.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if self.threshold is None:\n",
    "            threshold = tf.reduce_max(y_pred, axis=-1, keepdims=True)\n",
    "            # make sure [0, 0, 0] doesn't become [1, 1, 1]\n",
    "            # Use abs(x) > eps, instead of x != 0 to check for zero\n",
    "            y_pred = tf.logical_and(y_pred >= threshold,\n",
    "                                    tf.abs(y_pred) > 1e-12)\n",
    "        else:\n",
    "            y_pred = y_pred > self.threshold\n",
    "\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_pred = tf.cast(y_pred, tf.int32)\n",
    "\n",
    "        def _count_non_zero(val):\n",
    "            non_zeros = tf.math.count_nonzero(val, axis=self.axis)\n",
    "            return tf.cast(non_zeros, self.dtype)\n",
    "\n",
    "        self.true_positives.assign_add(_count_non_zero(y_pred * y_true))\n",
    "        self.false_positives.assign_add(_count_non_zero(y_pred * (y_true - 1)))\n",
    "        self.false_negatives.assign_add(_count_non_zero((y_pred - 1) * y_true))\n",
    "        self.weights_intermediate.assign_add(_count_non_zero(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        precision = tf.math.divide_no_nan(\n",
    "            self.true_positives, self.true_positives + self.false_positives)\n",
    "        recall = tf.math.divide_no_nan(\n",
    "            self.true_positives, self.true_positives + self.false_negatives)\n",
    "\n",
    "        mul_value = precision * recall\n",
    "        add_value = (tf.math.square(self.beta) * precision) + recall\n",
    "        mean = (tf.math.divide_no_nan(mul_value, add_value))\n",
    "        f1_score = mean * (1 + tf.math.square(self.beta))\n",
    "\n",
    "        if self.average == 'weighted':\n",
    "            weights = tf.math.divide_no_nan(\n",
    "                self.weights_intermediate,\n",
    "                tf.reduce_sum(self.weights_intermediate))\n",
    "            f1_score = tf.reduce_sum(f1_score * weights)\n",
    "\n",
    "        elif self.average is not None:  # [micro, macro]\n",
    "            f1_score = tf.reduce_mean(f1_score)\n",
    "\n",
    "        return f1_score\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Returns the serializable config of the metric.\"\"\"\n",
    "\n",
    "        config = {\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"average\": self.average,\n",
    "            \"beta\": self.beta,\n",
    "        }\n",
    "\n",
    "        if self.threshold is not None:\n",
    "            config[\"threshold\"] = self.threshold\n",
    "\n",
    "        base_config = super(FBetaScore, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(tf.zeros(self.init_shape, self.dtype))\n",
    "        self.false_positives.assign(tf.zeros(self.init_shape, self.dtype))\n",
    "        self.false_negatives.assign(tf.zeros(self.init_shape, self.dtype))\n",
    "        self.weights_intermediate.assign(tf.zeros(self.init_shape, self.dtype))\n",
    "class Mish(Activation):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "get_custom_objects().update({'Mish': Mish(mish)})\n",
    "\n",
    "class macroF1(Callback):\n",
    "    def __init__(self, model, inputs, targets, step = 10):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "        self.step = step\n",
    "\n",
    "    def on_train_begin(self, logs={}):    \n",
    "            # Initialization code    \n",
    "            self.epochs = 0    \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.epochs += 1   \n",
    "        if self.epochs % self.step == 0:  \n",
    "          pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "          f1_val = f1_score(self.targets, pred, average=\"macro\")\n",
    "          print(\"Step: {}  --  val_f1_macro_score: {}\".format(self.epochs, f1_val))\n",
    "def Classifier(shape_ = (LEN_, 3)):\n",
    "    \n",
    "    def bilstm(x, n_units):\n",
    "        x = L.Bidirectional(L.GRU(n_units, return_sequences=True))(x)\n",
    "        return(x)\n",
    "\n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [kernel_size**i for i in range(n)]\n",
    "        x = layers.Conv1D(filters = filters,\n",
    "                   kernel_size = 1,\n",
    "                   padding = 'same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = layers.Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same', \n",
    "                              activation = 'tanh', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            sigm_out = layers.Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same',\n",
    "                              activation = 'sigmoid', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            x = layers.Multiply()([tanh_out, sigm_out])\n",
    "            x = layers.Conv1D(filters = filters,\n",
    "                       kernel_size = 1,\n",
    "                       padding = 'same')(x)\n",
    "            res_x = layers.Add()([res_x, x])\n",
    "            \n",
    "        return res_x\n",
    "    \n",
    "    inp = layers.Input(shape = (shape_))\n",
    "    \n",
    "    x = wave_block(inp, 16, 3, 6)\n",
    "    x = wave_block(x, 32, 3, 4)\n",
    "    x = wave_block(x, 32, 3, 4)\n",
    "    x = wave_block(x, 64, 3, 2)\n",
    "    x = wave_block(x, 64, 3, 2)\n",
    "    x = wave_block(x, 128, 3, 1)\n",
    "\n",
    "    x = bilstm(x, 64)\n",
    "    x = L.Dropout(0.1, noise_shape = (None, 1, 128))(x)\n",
    "\n",
    "    x = L.TimeDistributed(L.Dense(128, activation = 'Mish'), name = 'first_distributed')(x)\n",
    "    x = L.Dropout(0.1, noise_shape = (None, 1, 128))(x)\n",
    "    \n",
    "    x = L.TimeDistributed(L.Dense(64, activation = 'Mish'), name = 'second_distributed')(x)\n",
    "    out = L.TimeDistributed(L.Dense(n_classes, activation = 'softmax'), name = 'output')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    model.compile(Adam(learning_rate = learning_rate), loss = [tfa.losses.SigmoidFocalCrossEntropy()], \n",
    "          metrics=[FBetaScore(n_classes, average = \"macro\")])\n",
    "    return model\n",
    "\n",
    "def load_trained_model(weights_path, shape_):\n",
    "    modelf = Classifier(shape_)\n",
    "    modelf.load_weights(weights_path)\n",
    "    \n",
    "    return(modelf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input/kalman-add/'\n",
    "\n",
    "group = np.load(path + 'group.npy', allow_pickle = True)\n",
    "test_signal = np.load(path + 'test_signal.npy', allow_pickle = True)\n",
    "train_signal = np.load(path + 'train_signal.npy', allow_pickle = True)\n",
    "train_target = np.load(path + 'train_target.npy', allow_pickle = True)\n",
    "model = Classifier((None, train_signal.shape[-1]))\n",
    "\n",
    "pred_ = np.load('../input/wavenet-kalman-clean-2/pred.npy', allow_pickle = True)\n",
    "pred_o = np.load('../input/wavenet-without-out/pred.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# WAVENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN FOLD:     0 -------\n",
      "\n",
      "\n",
      "\n",
      "BEGIN FOLD:     1 -------\n",
      "\n",
      "\n",
      "\n",
      "BEGIN FOLD:     2 -------\n",
      "\n",
      "\n",
      "\n",
      "BEGIN FOLD:     3 -------\n",
      "\n",
      "\n",
      "\n",
      "BEGIN FOLD:     4 -------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "kf = GroupKFold(n_splits = folds)\n",
    "pred_nn = np.zeros((train_signal.shape[0], train_signal.shape[1], 11))\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf.split(train_signal, train_target, group)):\n",
    "    print(f'BEGIN FOLD:     {fold_n} -------\\n\\n\\n')\n",
    "\n",
    "    X_train, X_valid = train_signal[train_index,:,:], train_signal[valid_index,:,:]\n",
    "    y_train, y_valid = train_target[train_index,:,:], train_target[valid_index,:,:]\n",
    "\n",
    "    model = load_trained_model(\"../input/wavenet-kalman-clean-2/model\" + str(fold_n) + \".hdf5\", (None, train_signal.shape[-1]))\n",
    "    \n",
    "    pred_nn[valid_index, :, :] = model.predict(X_valid)\n",
    "\n",
    "del X_train, X_valid, y_train, y_valid, model\n",
    "gc.collect()\n",
    "\n",
    "pred_nn = pred_nn.reshape((-1, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pred_wavenet_oof.npy', pred_nn, allow_pickle = True)\n",
    "np.save('pred_wavenet.npy', pred_, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# WAVENET NO OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = np.arange(910, 956)\n",
    "idx = np.arange(train_signal.shape[0])\n",
    "bool_mask = np.isin(idx, out_)\n",
    "train_signal = train_signal[~bool_mask]\n",
    "group = group[~bool_mask]\n",
    "train_target = train_target[~bool_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN FOLD:     0 -------\n",
      "\n",
      "\n",
      "\n",
      "BEGIN FOLD:     1 -------\n",
      "\n",
      "\n",
      "\n",
      "BEGIN FOLD:     2 -------\n",
      "\n",
      "\n",
      "\n",
      "BEGIN FOLD:     3 -------\n",
      "\n",
      "\n",
      "\n",
      "BEGIN FOLD:     4 -------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "kf = GroupKFold(n_splits = folds)\n",
    "pred_nn = np.zeros((train_signal.shape[0], train_signal.shape[1], 11))\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf.split(train_signal, train_target, group)):\n",
    "    print(f'BEGIN FOLD:     {fold_n} -------\\n\\n\\n')\n",
    "\n",
    "    X_train, X_valid = train_signal[train_index,:,:], train_signal[valid_index,:,:]\n",
    "    y_train, y_valid = train_target[train_index,:,:], train_target[valid_index,:,:]\n",
    "\n",
    "    model = load_trained_model(\"../input/wavenet-without-out/model\" + str(fold_n) + \".hdf5\", (None, train_signal.shape[-1]))\n",
    "    \n",
    "    pred_nn[valid_index, :, :] = model.predict(X_valid)\n",
    "\n",
    "del X_train, X_valid, y_train, y_valid, train_signal, train_target, model\n",
    "gc.collect()\n",
    "\n",
    "pred_nn = pred_nn.reshape((-1, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pred_wavenet_no_out_oof.npy', pred_nn, allow_pickle = True)\n",
    "np.save('pred_wavenet_no_out.npy', pred_o, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(folds):\n",
    "    path = f'../input/bilstm-final-fold{i}/'\n",
    "    if i == 0:\n",
    "        pred = np.load(path + f'pred{i}.npy', allow_pickle = True)/folds\n",
    "        pred_oof = np.load(path + f'predoof{i}.npy', allow_pickle = True)\n",
    "    else:\n",
    "        pred += np.load(path + f'pred{i}.npy', allow_pickle = True)/folds\n",
    "        pred_oof += np.load(path + f'predoof{i}.npy', allow_pickle = True)\n",
    "\n",
    "pred_oof = pred_oof.reshape((-1, n_classes))\n",
    "pred = pred.reshape((-1, n_classes))\n",
    "\n",
    "np.save('pred_bilstm_oof.npy', pred_oof, allow_pickle = True)\n",
    "np.save('pred_bilstm.npy', pred, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# BILSTM NO-OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(folds):\n",
    "    path = f'../input/bilstm-noout-fold{i}/'\n",
    "    if i == 0:\n",
    "        pred = np.load(path + f'pred{i}.npy', allow_pickle = True)/folds\n",
    "        pred_oof = np.load(path + f'predoof{i}.npy', allow_pickle = True)\n",
    "    else:\n",
    "        pred += np.load(path + f'pred{i}.npy', allow_pickle = True)/folds\n",
    "        pred_oof += np.load(path + f'predoof{i}.npy', allow_pickle = True)\n",
    "\n",
    "pred_oof = pred_oof.reshape((-1, n_classes))\n",
    "pred = pred.reshape((-1, n_classes))\n",
    "\n",
    "np.save('pred_bilstm_noout_oof.npy', pred_oof, allow_pickle = True)\n",
    "np.save('pred_bilstm_noout.npy', pred, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\n",
    "\n",
    "sub['open_channels'] = np.argmax(pred, axis = 1).reshape(-1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
