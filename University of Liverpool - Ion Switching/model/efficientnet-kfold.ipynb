{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":1,"outputs":[{"output_type":"stream","text":"2.1.0\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import copy\nimport math\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras import Model\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping, ModelCheckpoint\nimport math\n# from tqdm.notebook import tqdm\nimport gc\nimport os\nfrom sklearn.metrics import recall_score, f1_score, precision_score\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom sklearn.model_selection import KFold\nfrom tensorflow.python.keras.engine import training\n\nLEN_ = 4000\nn_epoch = 200\nbatch_size = 32\nSTEP_ = LEN_\nfolds = 5\nlearning_rate = 0.001\nSEED_ = 987654321\npatience = 20\nFOLD_ = 0\n\nlr_schedule = LearningRateScheduler(\n    PolynomialDecay(\n      initial_learning_rate = learning_rate,\n      decay_steps = n_epoch,\n      end_learning_rate = 0.0\n    )\n)\n\nearly_stopping = EarlyStopping(patience = patience, verbose=0)\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Mish(Activation):\n    '''\n    Mish Activation Function.\n    .. math::\n        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n    Shape:\n        - Input: Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n        - Output: Same shape as the input.\n    Examples:\n        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n    '''\n\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\n\n\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n\nget_custom_objects().update({'Mish': Mish(mish)})\n\nclass macroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = targets.reshape(-1)\n\n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n        f1_val = f1_score(self.targets, pred, average=\"macro\")\n        print(\"  --  val_f1_macro_score: \", f1_val)\n        \n\ndef load_trained_model(weights_path):\n    modelf = Unet_model()\n    modelf.load_weights(weights_path)\n    \n    return(modelf)\n\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEFAULT_BLOCKS_ARGS = [{\n    'kernel_size': 3,\n    'repeats': 1,\n    'filters_in': 32,\n    'filters_out': 16,\n    'expand_ratio': 1,\n    'id_skip': True,\n    'strides': 1,\n    'se_ratio': 0.25\n}, {\n    'kernel_size': 3,\n    'repeats': 2,\n    'filters_in': 16,\n    'filters_out': 24,\n    'expand_ratio': 6,\n    'id_skip': True,\n    'strides': 2,\n    'se_ratio': 0.25\n}, {\n    'kernel_size': 5,\n    'repeats': 2,\n    'filters_in': 24,\n    'filters_out': 40,\n    'expand_ratio': 6,\n    'id_skip': True,\n    'strides': 2,\n    'se_ratio': 0.25\n}, {\n    'kernel_size': 3,\n    'repeats': 3,\n    'filters_in': 40,\n    'filters_out': 80,\n    'expand_ratio': 6,\n    'id_skip': True,\n    'strides': 2,\n    'se_ratio': 0.25\n}, {\n    'kernel_size': 5,\n    'repeats': 3,\n    'filters_in': 80,\n    'filters_out': 112,\n    'expand_ratio': 6,\n    'id_skip': True,\n    'strides': 1,\n    'se_ratio': 0.25\n}, {\n    'kernel_size': 5,\n    'repeats': 4,\n    'filters_in': 112,\n    'filters_out': 192,\n    'expand_ratio': 6,\n    'id_skip': True,\n    'strides': 2,\n    'se_ratio': 0.25\n}, {\n    'kernel_size': 3,\n    'repeats': 1,\n    'filters_in': 192,\n    'filters_out': 320,\n    'expand_ratio': 6,\n    'id_skip': True,\n    'strides': 1,\n    'se_ratio': 0.25\n}]\n\nCONV_KERNEL_INITIALIZER = {\n    'class_name': 'VarianceScaling',\n    'config': {\n        'scale': 2.0,\n        'mode': 'fan_out',\n        'distribution': 'truncated_normal'\n    }\n}\n\nDENSE_KERNEL_INITIALIZER = {\n    'class_name': 'VarianceScaling',\n    'config': {\n        'scale': 1. / 3.,\n        'mode': 'fan_out',\n        'distribution': 'uniform'\n    }\n}\n\n\ndef EfficientNet(\n    width_coefficient,\n    depth_coefficient,\n    dropout_rate = 0.2,\n    drop_connect_rate = 0.2,\n    depth_divisor = 8,\n    activation = 'Mish',\n    blocks_args = 'default',\n    model_name = 'efficientnet',\n    include_top = True,\n    input_shape = None,\n    pooling = None,\n    classes = 11,\n    classifier_activation='softmax'):\n\n  \"\"\"Instantiates the EfficientNet architecture using given scaling coefficients.\n  Reference paper:\n  - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks]\n    (https://arxiv.org/abs/1905.11946) (ICML 2019)\n  Optionally loads weights pre-trained on ImageNet.\n  Note that the data format convention used by the model is\n  the one specified in your Keras config at `~/.keras/keras.json`.\n  Arguments:\n    width_coefficient: float, scaling coefficient for network width.\n    depth_coefficient: float, scaling coefficient for network depth.\n    default_size: integer, default input image size.\n    dropout_rate: float, dropout rate before final classifier layer.\n    drop_connect_rate: float, dropout rate at skip connections.\n    depth_divisor: integer, a unit of network width.\n    activation: activation function.\n    blocks_args: list of dicts, parameters to construct block modules.\n    model_name: string, model name.\n    include_top: whether to include the fully-connected\n        layer at the top of the network.\n    weights: one of `None` (random initialization),\n          'imagenet' (pre-training on ImageNet),\n          or the path to the weights file to be loaded.\n    input_tensor: optional Keras tensor\n        (i.e. output of `layers.Input()`)\n        to use as image input for the model.\n    input_shape: optional shape tuple, only to be specified\n        if `include_top` is False.\n        It should have exactly 3 inputs channels.\n    pooling: optional pooling mode for feature extraction\n        when `include_top` is `False`.\n        - `None` means that the output of the model will be\n            the 4D tensor output of the\n            last convolutional layer.\n        - `avg` means that global average pooling\n            will be applied to the output of the\n            last convolutional layer, and thus\n            the output of the model will be a 2D tensor.\n        - `max` means that global max pooling will\n            be applied.\n    classes: optional number of classes to classify images\n        into, only to be specified if `include_top` is True, and\n        if no `weights` argument is specified.\n    classifier_activation: A `str` or callable. The activation function to use\n        on the \"top\" layer. Ignored unless `include_top=True`. Set\n        `classifier_activation=None` to return the logits of the \"top\" layer.\n  Returns:\n    A `keras.Model` instance.\n  Raises:\n    ValueError: in case of invalid argument for `weights`,\n      or invalid input shape.\n    ValueError: if `classifier_activation` is not `softmax` or `None` when\n      using a pretrained top layer.\n  \"\"\"\n\n  if blocks_args == 'default':\n    blocks_args = DEFAULT_BLOCKS_ARGS\n   \n  input_layer = layers.Input(shape=input_shape)\n\n  bn_axis = 2\n\n  def round_filters(filters, divisor=depth_divisor):\n    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n    filters *= width_coefficient\n    new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_filters < 0.9 * filters:\n      new_filters += divisor\n    return int(new_filters)\n\n  def round_repeats(repeats):\n    \"\"\"Round number of repeats based on depth multiplier.\"\"\"\n    return int(math.ceil(depth_coefficient * repeats))\n\n  # Build stem\n  x = input_layer\n  x = layers.Conv1D(\n      round_filters(32),\n      3,\n      strides=2,\n      padding='valid',\n      use_bias=False,\n      kernel_initializer=CONV_KERNEL_INITIALIZER,\n      name='stem_conv')(x)\n  x = layers.BatchNormalization(axis=bn_axis, name='stem_bn')(x)\n  x = layers.Activation(activation, name='stem_activation')(x)\n\n  # Build blocks\n  blocks_args = copy.deepcopy(blocks_args)\n\n  b = 0\n  blocks = float(sum(args['repeats'] for args in blocks_args))\n  for (i, args) in enumerate(blocks_args):\n    assert args['repeats'] > 0\n    # Update block input and output filters based on depth multiplier.\n    args['filters_in'] = round_filters(args['filters_in'])\n    args['filters_out'] = round_filters(args['filters_out'])\n\n    for j in range(round_repeats(args.pop('repeats'))):\n      # The first block needs to take care of stride and filter size increase.\n      if j > 0:\n        args['strides'] = 1\n        args['filters_in'] = args['filters_out']\n      x = block(\n          x,\n          activation,\n          drop_connect_rate * b / blocks,\n          name='block{}{}_'.format(i + 1, chr(j + 97)),\n          **args)\n      b += 1\n\n  # Build top\n  x = layers.Conv1D(\n      round_filters(1280),\n      1,\n      padding='same',\n      use_bias=False,\n      kernel_initializer=CONV_KERNEL_INITIALIZER,\n      name='top_conv')(x)\n  x = layers.BatchNormalization(axis=bn_axis, name='top_bn')(x)\n  x = layers.Activation(activation, name='top_activation')(x)\n  if include_top:\n    x = layers.GlobalAveragePooling1D(name='avg_pool')(x)\n    if dropout_rate > 0:\n      x = layers.Dropout(dropout_rate, name='top_dropout')(x)\n    x = layers.Dense(\n        classes,\n        activation=classifier_activation,\n        kernel_initializer=DENSE_KERNEL_INITIALIZER,\n        name='predictions')(x)\n  else:\n    if pooling == 'avg':\n      x = layers.GlobalAveragePooling1D(name='avg_pool')(x)\n    elif pooling == 'max':\n      x = layers.GlobalMaxPooling1D(name='max_pool')(x)\n\n  # Create model.\n  model = training.Model(input_layer, x, name=model_name)\n\n  return model\n\n\ndef block(inputs,\n          activation='Mish',\n          drop_rate=0.,\n          name='',\n          filters_in=32,\n          filters_out=16,\n          kernel_size=3,\n          strides=1,\n          expand_ratio=1,\n          se_ratio=0.,\n          id_skip=True):\n  \"\"\"An inverted residual block.\n  Arguments:\n      inputs: input tensor.\n      activation: activation function.\n      drop_rate: float between 0 and 1, fraction of the input units to drop.\n      name: string, block label.\n      filters_in: integer, the number of input filters.\n      filters_out: integer, the number of output filters.\n      kernel_size: integer, the dimension of the convolution window.\n      strides: integer, the stride of the convolution.\n      expand_ratio: integer, scaling coefficient for the input filters.\n      se_ratio: float between 0 and 1, fraction to squeeze the input filters.\n      id_skip: boolean.\n  Returns:\n      output tensor for the block.\n  \"\"\"\n  bn_axis = 2\n  conv_pad = 'valid'\n\n  # Expansion phase\n  filters = filters_in * expand_ratio\n  if expand_ratio != 1:\n    x = layers.Conv1D(\n        filters,\n        1,\n        padding='same',\n        use_bias=False,\n        kernel_initializer=CONV_KERNEL_INITIALIZER,\n        name=name + 'expand_conv')(\n            inputs)\n    x = layers.BatchNormalization(axis=bn_axis, name=name + 'expand_bn')(x)\n    x = layers.Activation(activation, name=name + 'expand_activation')(x)\n  else:\n    x = inputs\n\n  x = layers.SeparableConv1D(\n      filters,\n      kernel_size,\n      strides=strides,\n      padding='same',\n      use_bias=False,\n      kernel_initializer=CONV_KERNEL_INITIALIZER,\n      name=name + 'dwconv')(x)\n  x = layers.BatchNormalization(axis=bn_axis, name=name + 'bn')(x)\n  x = layers.Activation(activation, name=name + 'activation')(x)\n\n  # Squeeze and Excitation phase\n  if 0 < se_ratio <= 1:\n    filters_se = max(1, int(filters_in * se_ratio))\n    se = layers.GlobalAveragePooling1D(name=name + 'se_squeeze')(x)\n    se = layers.Reshape((1, filters), name=name + 'se_reshape')(se)\n    se = layers.Conv1D(\n        filters_se,\n        1,\n        padding='same',\n        activation=activation,\n        kernel_initializer=CONV_KERNEL_INITIALIZER,\n        name=name + 'se_reduce')(\n            se)\n    se = layers.Conv1D(\n        filters,\n        1,\n        padding='same',\n        activation='sigmoid',\n        kernel_initializer=CONV_KERNEL_INITIALIZER,\n        name=name + 'se_expand')(se)\n    x = layers.multiply([x, se], name=name + 'se_excite')\n\n  # Output phase\n  x = layers.Conv1D(\n      filters_out,\n      1,\n      padding='same',\n      use_bias=False,\n      kernel_initializer=CONV_KERNEL_INITIALIZER,\n      name=name + 'project_conv')(x)\n  x = layers.BatchNormalization(axis=bn_axis, name=name + 'project_bn')(x)\n  if id_skip and strides == 1 and filters_in == filters_out:\n    if drop_rate > 0:\n      x = layers.Dropout(\n          drop_rate, noise_shape=(None, 1, 1), name=name + 'drop')(x)\n    x = layers.add([x, inputs], name=name + 'add')\n  return x\n\n\n\ndef EfficientNetB5(include_top=True,\n                   input_tensor=None,\n                   input_shape=None,\n                   pooling=None,\n                   classes=11,\n                   **kwargs):\n  return EfficientNet(\n      1.6,\n      2.2,\n      456,\n      0.4,\n      model_name='efficientnetb5',\n      include_top=include_top,\n      input_shape=input_shape,\n      pooling=pooling,\n      classes=classes,\n      **kwargs)\n\n\ndef EfficientNetB6(include_top=True,\n                   input_shape=None,\n                   pooling=None,\n                   classes=11,\n                   **kwargs):\n  return EfficientNet(\n      1.8,\n      2.6,\n      0.5,\n      model_name='efficientnetb6',\n      include_top=include_top,\n      input_shape=input_shape,\n      pooling=pooling,\n      classes=classes,\n      **kwargs)\n\ndef EfficientNetB0(include_top=True,\n                   input_shape=None,\n                   pooling=None,\n                   classes=11,\n                   **kwargs):\n  return EfficientNet(\n      1.0,\n      1.0,\n      0.2,\n      model_name='efficientnetb0',\n      include_top=include_top,\n      input_shape=input_shape,\n      pooling=pooling,\n      classes=classes,\n      **kwargs)\n\ndef EfficientNetB7(include_top=True,\n                   input_shape=None,\n                   pooling=None,\n                   classes=11,\n                   **kwargs):\n    \n    return EfficientNet(\n      2.0,\n      3.1,\n      0.5,\n      model_name='efficientnetb7',\n      include_top=include_top,\n      input_shape=input_shape,\n      pooling=pooling,\n      classes=classes,\n      **kwargs)\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Conv2dBn(\n        filters,\n        kernel_size,\n        activation='Mish',\n        strides = 1,\n        kernel_initializer='he_uniform',\n        padding='valid'):\n    \"\"\"Extension of Conv2D layer with batchnorm\"\"\"\n\n    bn_axis = 2\n\n    def wrapper(input_tensor):\n\n        x = layers.Conv1D(\n              filters,\n              kernel_size,\n              strides=strides,\n              padding=padding)(input_tensor)\n        x = layers.BatchNormalization(axis=bn_axis, name='stem_bn')(x)\n        x = layers.Activation(activation, name='stem_activation')(x)\n\n        return x\n\n    return wrapper\n\ndef Conv3x3BnMish(filters, use_batchnorm, name=None):\n\n    def wrapper(input_tensor):\n        return Conv2dBn(\n            filters,\n            kernel_size=3,\n            activation='Mish',\n            kernel_initializer='he_uniform',\n            padding='same',\n        )(input_tensor)\n\n    return wrapper\n\n\ndef DecoderUpsamplingX2Block(filters, stage, use_batchnorm=False):\n    up_name = 'decoder_stage{}_upsampling'.format(stage)\n    conv1_name = 'decoder_stage{}a'.format(stage)\n    conv2_name = 'decoder_stage{}b'.format(stage)\n    concat_name = 'decoder_stage{}_concat'.format(stage)\n\n    concat_axis = 2\n    \n    def wrapper(input_tensor, skip=None):\n        x = layers.UpSampling1D(size=2, name=up_name)(input_tensor)\n\n        if skip is not None:\n            x = layers.Concatenate(axis=concat_axis, name=concat_name)([x, skip])\n\n        x = Conv3x3BnMish(filters, use_batchnorm, name=conv1_name)(x)\n        x = Conv3x3BnMish(filters, use_batchnorm, name=conv2_name)(x)\n\n        return x\n\n    return wrapper\n\nfeature_layer = ('block6a_expand_activation', 'block4a_expand_activation',\n                   'block3a_expand_activation', 'block2a_expand_activation')\n\n# ---------------------------------------------------------------------\n#  Unet Decoder\n# ---------------------------------------------------------------------\n\ndef build_unet(\n        backbone,\n        decoder_block = DecoderUpsamplingX2Block,\n        skip_connection_layers = feature_layer,\n        decoder_filters = (256, 128, 64, 32, 16),\n        n_upsample_blocks=5,\n        classes=1,\n        activation='sigmoid',\n        use_batchnorm=True,\n):\n    input_ = backbone.input\n    x = backbone.output\n\n    # extract skip connections\n    skips = ([backbone.get_layer(name=i).output for i in skip_connection_layers])\n\n    # add center block if previous operation was maxpooling (for vgg models)\n    if isinstance(backbone.layers[-1], layers.MaxPooling1D):\n        x = Conv3x3BnReLU(512, use_batchnorm, name='center_block1')(x)\n        x = Conv3x3BnReLU(512, use_batchnorm, name='center_block2')(x)\n\n    # building decoder blocks\n    for i in range(n_upsample_blocks):\n\n        if i < len(skips):\n            skip = skips[i]\n        else:\n            skip = None\n\n        x = decoder_block(decoder_filters[i], stage=i, use_batchnorm=use_batchnorm)(x, skip)\n\n    # model head (define number of output classes)\n    x = layers.Conv1D(\n        filters=classes,\n        kernel_size=(3, 3),\n        padding='same',\n        use_bias=True,\n        kernel_initializer='glorot_uniform',\n        name='final_conv',\n    )(x)\n    x = layers.Activation(activation, name=activation)(x)\n\n    # create keras model instance\n    model = models.Model(input_, x)\n\n    return model\n","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# backbone = EfficientNetB0(include_top=False, input_shape = (LEN_, 1))\nmodel = build_unet(backbone)","execution_count":33,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 2000, 64), (None, 1999, 96)]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-2af835a31c6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# backbone = EfficientNetB0(include_top=False, input_shape = (LEN_, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-365a6ad1b9d5>\u001b[0m in \u001b[0;36mbuild_unet\u001b[0;34m(backbone, decoder_block, skip_connection_layers, decoder_filters, n_upsample_blocks, classes, activation, use_batchnorm)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_filters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_batchnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_batchnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# model head (define number of output classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-365a6ad1b9d5>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(input_tensor, skip)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv3x3BnMish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_batchnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv1_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2114\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    399\u001b[0m                            if shape[axis] is not None])\n\u001b[1;32m    400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 2000, 64), (None, 1999, 96)]"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":10,"outputs":[{"output_type":"stream","text":"Model: \"efficientnetb0\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 4000, 1)]    0                                            \n__________________________________________________________________________________________________\nstem_conv (Conv1D)              (None, 1999, 32)     96          input_2[0][0]                    \n__________________________________________________________________________________________________\nstem_bn (BatchNormalization)    (None, 1999, 32)     128         stem_conv[0][0]                  \n__________________________________________________________________________________________________\nstem_activation (Activation)    (None, 1999, 32)     0           stem_bn[0][0]                    \n__________________________________________________________________________________________________\nblock1a_dwconv (SeparableConv1D (None, 1999, 32)     1120        stem_activation[0][0]            \n__________________________________________________________________________________________________\nblock1a_bn (BatchNormalization) (None, 1999, 32)     128         block1a_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock1a_activation (Activation) (None, 1999, 32)     0           block1a_bn[0][0]                 \n__________________________________________________________________________________________________\nblock1a_se_squeeze (GlobalAvera (None, 32)           0           block1a_activation[0][0]         \n__________________________________________________________________________________________________\nblock1a_se_reshape (Reshape)    (None, 1, 32)        0           block1a_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock1a_se_reduce (Conv1D)      (None, 1, 8)         264         block1a_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock1a_se_expand (Conv1D)      (None, 1, 32)        288         block1a_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock1a_se_excite (Multiply)    (None, 1999, 32)     0           block1a_activation[0][0]         \n                                                                 block1a_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock1a_project_conv (Conv1D)   (None, 1999, 16)     512         block1a_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock1a_project_bn (BatchNormal (None, 1999, 16)     64          block1a_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock2a_expand_conv (Conv1D)    (None, 1999, 96)     1536        block1a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock2a_expand_bn (BatchNormali (None, 1999, 96)     384         block2a_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock2a_expand_activation (Acti (None, 1999, 96)     0           block2a_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock2a_dwconv (SeparableConv1D (None, 1000, 96)     9504        block2a_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock2a_bn (BatchNormalization) (None, 1000, 96)     384         block2a_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock2a_activation (Activation) (None, 1000, 96)     0           block2a_bn[0][0]                 \n__________________________________________________________________________________________________\nblock2a_se_squeeze (GlobalAvera (None, 96)           0           block2a_activation[0][0]         \n__________________________________________________________________________________________________\nblock2a_se_reshape (Reshape)    (None, 1, 96)        0           block2a_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock2a_se_reduce (Conv1D)      (None, 1, 4)         388         block2a_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock2a_se_expand (Conv1D)      (None, 1, 96)        480         block2a_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock2a_se_excite (Multiply)    (None, 1000, 96)     0           block2a_activation[0][0]         \n                                                                 block2a_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock2a_project_conv (Conv1D)   (None, 1000, 24)     2304        block2a_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock2a_project_bn (BatchNormal (None, 1000, 24)     96          block2a_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock2b_expand_conv (Conv1D)    (None, 1000, 144)    3456        block2a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock2b_expand_bn (BatchNormali (None, 1000, 144)    576         block2b_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock2b_expand_activation (Acti (None, 1000, 144)    0           block2b_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock2b_dwconv (SeparableConv1D (None, 1000, 144)    21168       block2b_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock2b_bn (BatchNormalization) (None, 1000, 144)    576         block2b_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock2b_activation (Activation) (None, 1000, 144)    0           block2b_bn[0][0]                 \n__________________________________________________________________________________________________\nblock2b_se_squeeze (GlobalAvera (None, 144)          0           block2b_activation[0][0]         \n__________________________________________________________________________________________________\nblock2b_se_reshape (Reshape)    (None, 1, 144)       0           block2b_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock2b_se_reduce (Conv1D)      (None, 1, 6)         870         block2b_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock2b_se_expand (Conv1D)      (None, 1, 144)       1008        block2b_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock2b_se_excite (Multiply)    (None, 1000, 144)    0           block2b_activation[0][0]         \n                                                                 block2b_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock2b_project_conv (Conv1D)   (None, 1000, 24)     3456        block2b_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock2b_project_bn (BatchNormal (None, 1000, 24)     96          block2b_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock2b_drop (Dropout)          (None, 1000, 24)     0           block2b_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock2b_add (Add)               (None, 1000, 24)     0           block2b_drop[0][0]               \n                                                                 block2a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock3a_expand_conv (Conv1D)    (None, 1000, 144)    3456        block2b_add[0][0]                \n__________________________________________________________________________________________________\nblock3a_expand_bn (BatchNormali (None, 1000, 144)    576         block3a_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock3a_expand_activation (Acti (None, 1000, 144)    0           block3a_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock3a_dwconv (SeparableConv1D (None, 500, 144)     21456       block3a_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock3a_bn (BatchNormalization) (None, 500, 144)     576         block3a_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock3a_activation (Activation) (None, 500, 144)     0           block3a_bn[0][0]                 \n__________________________________________________________________________________________________\nblock3a_se_squeeze (GlobalAvera (None, 144)          0           block3a_activation[0][0]         \n__________________________________________________________________________________________________\nblock3a_se_reshape (Reshape)    (None, 1, 144)       0           block3a_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock3a_se_reduce (Conv1D)      (None, 1, 6)         870         block3a_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock3a_se_expand (Conv1D)      (None, 1, 144)       1008        block3a_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock3a_se_excite (Multiply)    (None, 500, 144)     0           block3a_activation[0][0]         \n                                                                 block3a_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock3a_project_conv (Conv1D)   (None, 500, 40)      5760        block3a_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock3a_project_bn (BatchNormal (None, 500, 40)      160         block3a_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock3b_expand_conv (Conv1D)    (None, 500, 240)     9600        block3a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock3b_expand_bn (BatchNormali (None, 500, 240)     960         block3b_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock3b_expand_activation (Acti (None, 500, 240)     0           block3b_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock3b_dwconv (SeparableConv1D (None, 500, 240)     58800       block3b_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock3b_bn (BatchNormalization) (None, 500, 240)     960         block3b_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock3b_activation (Activation) (None, 500, 240)     0           block3b_bn[0][0]                 \n__________________________________________________________________________________________________\nblock3b_se_squeeze (GlobalAvera (None, 240)          0           block3b_activation[0][0]         \n__________________________________________________________________________________________________\nblock3b_se_reshape (Reshape)    (None, 1, 240)       0           block3b_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock3b_se_reduce (Conv1D)      (None, 1, 10)        2410        block3b_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock3b_se_expand (Conv1D)      (None, 1, 240)       2640        block3b_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock3b_se_excite (Multiply)    (None, 500, 240)     0           block3b_activation[0][0]         \n                                                                 block3b_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock3b_project_conv (Conv1D)   (None, 500, 40)      9600        block3b_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock3b_project_bn (BatchNormal (None, 500, 40)      160         block3b_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock3b_drop (Dropout)          (None, 500, 40)      0           block3b_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock3b_add (Add)               (None, 500, 40)      0           block3b_drop[0][0]               \n                                                                 block3a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock4a_expand_conv (Conv1D)    (None, 500, 240)     9600        block3b_add[0][0]                \n__________________________________________________________________________________________________\nblock4a_expand_bn (BatchNormali (None, 500, 240)     960         block4a_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock4a_expand_activation (Acti (None, 500, 240)     0           block4a_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock4a_dwconv (SeparableConv1D (None, 250, 240)     58320       block4a_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock4a_bn (BatchNormalization) (None, 250, 240)     960         block4a_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock4a_activation (Activation) (None, 250, 240)     0           block4a_bn[0][0]                 \n__________________________________________________________________________________________________\nblock4a_se_squeeze (GlobalAvera (None, 240)          0           block4a_activation[0][0]         \n__________________________________________________________________________________________________\nblock4a_se_reshape (Reshape)    (None, 1, 240)       0           block4a_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock4a_se_reduce (Conv1D)      (None, 1, 10)        2410        block4a_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock4a_se_expand (Conv1D)      (None, 1, 240)       2640        block4a_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock4a_se_excite (Multiply)    (None, 250, 240)     0           block4a_activation[0][0]         \n                                                                 block4a_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock4a_project_conv (Conv1D)   (None, 250, 80)      19200       block4a_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock4a_project_bn (BatchNormal (None, 250, 80)      320         block4a_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock4b_expand_conv (Conv1D)    (None, 250, 480)     38400       block4a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock4b_expand_bn (BatchNormali (None, 250, 480)     1920        block4b_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock4b_expand_activation (Acti (None, 250, 480)     0           block4b_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock4b_dwconv (SeparableConv1D (None, 250, 480)     231840      block4b_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock4b_bn (BatchNormalization) (None, 250, 480)     1920        block4b_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock4b_activation (Activation) (None, 250, 480)     0           block4b_bn[0][0]                 \n__________________________________________________________________________________________________\nblock4b_se_squeeze (GlobalAvera (None, 480)          0           block4b_activation[0][0]         \n__________________________________________________________________________________________________\nblock4b_se_reshape (Reshape)    (None, 1, 480)       0           block4b_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock4b_se_reduce (Conv1D)      (None, 1, 20)        9620        block4b_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock4b_se_expand (Conv1D)      (None, 1, 480)       10080       block4b_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock4b_se_excite (Multiply)    (None, 250, 480)     0           block4b_activation[0][0]         \n                                                                 block4b_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock4b_project_conv (Conv1D)   (None, 250, 80)      38400       block4b_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock4b_project_bn (BatchNormal (None, 250, 80)      320         block4b_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock4b_drop (Dropout)          (None, 250, 80)      0           block4b_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock4b_add (Add)               (None, 250, 80)      0           block4b_drop[0][0]               \n                                                                 block4a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock4c_expand_conv (Conv1D)    (None, 250, 480)     38400       block4b_add[0][0]                \n__________________________________________________________________________________________________\nblock4c_expand_bn (BatchNormali (None, 250, 480)     1920        block4c_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock4c_expand_activation (Acti (None, 250, 480)     0           block4c_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock4c_dwconv (SeparableConv1D (None, 250, 480)     231840      block4c_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock4c_bn (BatchNormalization) (None, 250, 480)     1920        block4c_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock4c_activation (Activation) (None, 250, 480)     0           block4c_bn[0][0]                 \n__________________________________________________________________________________________________\nblock4c_se_squeeze (GlobalAvera (None, 480)          0           block4c_activation[0][0]         \n__________________________________________________________________________________________________\nblock4c_se_reshape (Reshape)    (None, 1, 480)       0           block4c_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock4c_se_reduce (Conv1D)      (None, 1, 20)        9620        block4c_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock4c_se_expand (Conv1D)      (None, 1, 480)       10080       block4c_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock4c_se_excite (Multiply)    (None, 250, 480)     0           block4c_activation[0][0]         \n                                                                 block4c_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock4c_project_conv (Conv1D)   (None, 250, 80)      38400       block4c_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock4c_project_bn (BatchNormal (None, 250, 80)      320         block4c_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock4c_drop (Dropout)          (None, 250, 80)      0           block4c_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock4c_add (Add)               (None, 250, 80)      0           block4c_drop[0][0]               \n                                                                 block4b_add[0][0]                \n__________________________________________________________________________________________________\nblock5a_expand_conv (Conv1D)    (None, 250, 480)     38400       block4c_add[0][0]                \n__________________________________________________________________________________________________\nblock5a_expand_bn (BatchNormali (None, 250, 480)     1920        block5a_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock5a_expand_activation (Acti (None, 250, 480)     0           block5a_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock5a_dwconv (SeparableConv1D (None, 250, 480)     232800      block5a_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock5a_bn (BatchNormalization) (None, 250, 480)     1920        block5a_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock5a_activation (Activation) (None, 250, 480)     0           block5a_bn[0][0]                 \n__________________________________________________________________________________________________\nblock5a_se_squeeze (GlobalAvera (None, 480)          0           block5a_activation[0][0]         \n__________________________________________________________________________________________________\nblock5a_se_reshape (Reshape)    (None, 1, 480)       0           block5a_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock5a_se_reduce (Conv1D)      (None, 1, 20)        9620        block5a_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock5a_se_expand (Conv1D)      (None, 1, 480)       10080       block5a_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock5a_se_excite (Multiply)    (None, 250, 480)     0           block5a_activation[0][0]         \n                                                                 block5a_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock5a_project_conv (Conv1D)   (None, 250, 112)     53760       block5a_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock5a_project_bn (BatchNormal (None, 250, 112)     448         block5a_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock5b_expand_conv (Conv1D)    (None, 250, 672)     75264       block5a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock5b_expand_bn (BatchNormali (None, 250, 672)     2688        block5b_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock5b_expand_activation (Acti (None, 250, 672)     0           block5b_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock5b_dwconv (SeparableConv1D (None, 250, 672)     454944      block5b_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock5b_bn (BatchNormalization) (None, 250, 672)     2688        block5b_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock5b_activation (Activation) (None, 250, 672)     0           block5b_bn[0][0]                 \n__________________________________________________________________________________________________\nblock5b_se_squeeze (GlobalAvera (None, 672)          0           block5b_activation[0][0]         \n__________________________________________________________________________________________________\nblock5b_se_reshape (Reshape)    (None, 1, 672)       0           block5b_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock5b_se_reduce (Conv1D)      (None, 1, 28)        18844       block5b_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock5b_se_expand (Conv1D)      (None, 1, 672)       19488       block5b_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock5b_se_excite (Multiply)    (None, 250, 672)     0           block5b_activation[0][0]         \n                                                                 block5b_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock5b_project_conv (Conv1D)   (None, 250, 112)     75264       block5b_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock5b_project_bn (BatchNormal (None, 250, 112)     448         block5b_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock5b_drop (Dropout)          (None, 250, 112)     0           block5b_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock5b_add (Add)               (None, 250, 112)     0           block5b_drop[0][0]               \n                                                                 block5a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock5c_expand_conv (Conv1D)    (None, 250, 672)     75264       block5b_add[0][0]                \n__________________________________________________________________________________________________\nblock5c_expand_bn (BatchNormali (None, 250, 672)     2688        block5c_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock5c_expand_activation (Acti (None, 250, 672)     0           block5c_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock5c_dwconv (SeparableConv1D (None, 250, 672)     454944      block5c_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock5c_bn (BatchNormalization) (None, 250, 672)     2688        block5c_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock5c_activation (Activation) (None, 250, 672)     0           block5c_bn[0][0]                 \n__________________________________________________________________________________________________\nblock5c_se_squeeze (GlobalAvera (None, 672)          0           block5c_activation[0][0]         \n__________________________________________________________________________________________________\nblock5c_se_reshape (Reshape)    (None, 1, 672)       0           block5c_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock5c_se_reduce (Conv1D)      (None, 1, 28)        18844       block5c_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock5c_se_expand (Conv1D)      (None, 1, 672)       19488       block5c_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock5c_se_excite (Multiply)    (None, 250, 672)     0           block5c_activation[0][0]         \n                                                                 block5c_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock5c_project_conv (Conv1D)   (None, 250, 112)     75264       block5c_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock5c_project_bn (BatchNormal (None, 250, 112)     448         block5c_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock5c_drop (Dropout)          (None, 250, 112)     0           block5c_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock5c_add (Add)               (None, 250, 112)     0           block5c_drop[0][0]               \n                                                                 block5b_add[0][0]                \n__________________________________________________________________________________________________\nblock6a_expand_conv (Conv1D)    (None, 250, 672)     75264       block5c_add[0][0]                \n__________________________________________________________________________________________________\nblock6a_expand_bn (BatchNormali (None, 250, 672)     2688        block6a_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock6a_expand_activation (Acti (None, 250, 672)     0           block6a_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock6a_dwconv (SeparableConv1D (None, 125, 672)     454944      block6a_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock6a_bn (BatchNormalization) (None, 125, 672)     2688        block6a_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock6a_activation (Activation) (None, 125, 672)     0           block6a_bn[0][0]                 \n__________________________________________________________________________________________________\nblock6a_se_squeeze (GlobalAvera (None, 672)          0           block6a_activation[0][0]         \n__________________________________________________________________________________________________\nblock6a_se_reshape (Reshape)    (None, 1, 672)       0           block6a_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock6a_se_reduce (Conv1D)      (None, 1, 28)        18844       block6a_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock6a_se_expand (Conv1D)      (None, 1, 672)       19488       block6a_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock6a_se_excite (Multiply)    (None, 125, 672)     0           block6a_activation[0][0]         \n                                                                 block6a_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock6a_project_conv (Conv1D)   (None, 125, 192)     129024      block6a_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock6a_project_bn (BatchNormal (None, 125, 192)     768         block6a_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock6b_expand_conv (Conv1D)    (None, 125, 1152)    221184      block6a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6b_expand_bn (BatchNormali (None, 125, 1152)    4608        block6b_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock6b_expand_activation (Acti (None, 125, 1152)    0           block6b_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock6b_dwconv (SeparableConv1D (None, 125, 1152)    1332864     block6b_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock6b_bn (BatchNormalization) (None, 125, 1152)    4608        block6b_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock6b_activation (Activation) (None, 125, 1152)    0           block6b_bn[0][0]                 \n__________________________________________________________________________________________________\nblock6b_se_squeeze (GlobalAvera (None, 1152)         0           block6b_activation[0][0]         \n__________________________________________________________________________________________________\nblock6b_se_reshape (Reshape)    (None, 1, 1152)      0           block6b_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock6b_se_reduce (Conv1D)      (None, 1, 48)        55344       block6b_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock6b_se_expand (Conv1D)      (None, 1, 1152)      56448       block6b_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock6b_se_excite (Multiply)    (None, 125, 1152)    0           block6b_activation[0][0]         \n                                                                 block6b_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock6b_project_conv (Conv1D)   (None, 125, 192)     221184      block6b_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock6b_project_bn (BatchNormal (None, 125, 192)     768         block6b_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock6b_drop (Dropout)          (None, 125, 192)     0           block6b_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6b_add (Add)               (None, 125, 192)     0           block6b_drop[0][0]               \n                                                                 block6a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6c_expand_conv (Conv1D)    (None, 125, 1152)    221184      block6b_add[0][0]                \n__________________________________________________________________________________________________\nblock6c_expand_bn (BatchNormali (None, 125, 1152)    4608        block6c_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock6c_expand_activation (Acti (None, 125, 1152)    0           block6c_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock6c_dwconv (SeparableConv1D (None, 125, 1152)    1332864     block6c_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock6c_bn (BatchNormalization) (None, 125, 1152)    4608        block6c_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock6c_activation (Activation) (None, 125, 1152)    0           block6c_bn[0][0]                 \n__________________________________________________________________________________________________\nblock6c_se_squeeze (GlobalAvera (None, 1152)         0           block6c_activation[0][0]         \n__________________________________________________________________________________________________\nblock6c_se_reshape (Reshape)    (None, 1, 1152)      0           block6c_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock6c_se_reduce (Conv1D)      (None, 1, 48)        55344       block6c_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock6c_se_expand (Conv1D)      (None, 1, 1152)      56448       block6c_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock6c_se_excite (Multiply)    (None, 125, 1152)    0           block6c_activation[0][0]         \n                                                                 block6c_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock6c_project_conv (Conv1D)   (None, 125, 192)     221184      block6c_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock6c_project_bn (BatchNormal (None, 125, 192)     768         block6c_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock6c_drop (Dropout)          (None, 125, 192)     0           block6c_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6c_add (Add)               (None, 125, 192)     0           block6c_drop[0][0]               \n                                                                 block6b_add[0][0]                \n__________________________________________________________________________________________________\nblock6d_expand_conv (Conv1D)    (None, 125, 1152)    221184      block6c_add[0][0]                \n__________________________________________________________________________________________________\nblock6d_expand_bn (BatchNormali (None, 125, 1152)    4608        block6d_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock6d_expand_activation (Acti (None, 125, 1152)    0           block6d_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock6d_dwconv (SeparableConv1D (None, 125, 1152)    1332864     block6d_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock6d_bn (BatchNormalization) (None, 125, 1152)    4608        block6d_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock6d_activation (Activation) (None, 125, 1152)    0           block6d_bn[0][0]                 \n__________________________________________________________________________________________________\nblock6d_se_squeeze (GlobalAvera (None, 1152)         0           block6d_activation[0][0]         \n__________________________________________________________________________________________________\nblock6d_se_reshape (Reshape)    (None, 1, 1152)      0           block6d_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock6d_se_reduce (Conv1D)      (None, 1, 48)        55344       block6d_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock6d_se_expand (Conv1D)      (None, 1, 1152)      56448       block6d_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock6d_se_excite (Multiply)    (None, 125, 1152)    0           block6d_activation[0][0]         \n                                                                 block6d_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock6d_project_conv (Conv1D)   (None, 125, 192)     221184      block6d_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock6d_project_bn (BatchNormal (None, 125, 192)     768         block6d_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock6d_drop (Dropout)          (None, 125, 192)     0           block6d_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6d_add (Add)               (None, 125, 192)     0           block6d_drop[0][0]               \n                                                                 block6c_add[0][0]                \n__________________________________________________________________________________________________\nblock7a_expand_conv (Conv1D)    (None, 125, 1152)    221184      block6d_add[0][0]                \n__________________________________________________________________________________________________\nblock7a_expand_bn (BatchNormali (None, 125, 1152)    4608        block7a_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock7a_expand_activation (Acti (None, 125, 1152)    0           block7a_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock7a_dwconv (SeparableConv1D (None, 125, 1152)    1330560     block7a_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock7a_bn (BatchNormalization) (None, 125, 1152)    4608        block7a_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock7a_activation (Activation) (None, 125, 1152)    0           block7a_bn[0][0]                 \n__________________________________________________________________________________________________\nblock7a_se_squeeze (GlobalAvera (None, 1152)         0           block7a_activation[0][0]         \n__________________________________________________________________________________________________\nblock7a_se_reshape (Reshape)    (None, 1, 1152)      0           block7a_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock7a_se_reduce (Conv1D)      (None, 1, 48)        55344       block7a_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock7a_se_expand (Conv1D)      (None, 1, 1152)      56448       block7a_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock7a_se_excite (Multiply)    (None, 125, 1152)    0           block7a_activation[0][0]         \n                                                                 block7a_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock7a_project_conv (Conv1D)   (None, 125, 320)     368640      block7a_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock7a_project_bn (BatchNormal (None, 125, 320)     1280        block7a_project_conv[0][0]       \n__________________________________________________________________________________________________\ntop_conv (Conv1D)               (None, 125, 1280)    409600      block7a_project_bn[0][0]         \n__________________________________________________________________________________________________\ntop_bn (BatchNormalization)     (None, 125, 1280)    5120        top_conv[0][0]                   \n__________________________________________________________________________________________________\ntop_activation (Activation)     (None, 125, 1280)    0           top_bn[0][0]                     \n==================================================================================================\nTotal params: 11,427,612\nTrainable params: 11,385,596\nNon-trainable params: 42,016\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/clean-removal-of-data-drift/train.csv\")\n_ = pd.read_csv(\"../input/liverpool-ion-switching/train.csv\")\ntrain['time'] = _['time']\ndel _\n\ntest = pd.read_csv(\"../input/clean-removal-of-data-drift/test.csv\")\n_ = pd.read_csv(\"../input/liverpool-ion-switching/test.csv\")\ntest['time'] = _['time']\ndel _\n\nn_classes = train.channel.unique().shape[0]\n\n\ntrain_mean = train.signal.mean()\ntrain_sigma = train.signal.std()\n\ntrain['signal'] = (train['signal'] - train_mean)/train_sigma\ntest['signal'] = (test['signal'] - train_mean)/train_sigma\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert train.shape[0] % LEN_ == 0, 'Check dimension of LEN_ in comparison to size of train. It need to be a multiple!'\n\ntrain_signal = train['signal'].values.reshape(-1, LEN_, 1)\ntrain_target = train['channel'].values.reshape(-1, LEN_, 1)\n\ntest_signal = test[\"signal\"].values.reshape(-1,LEN_,1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EfficientNetB0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"efficientnet = EfficientNetB0(weights = 'imagenet', include_top = False, classes = n_classes, input_shape = input_shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nkf = KFold(n_splits = folds, shuffle = True, random_state = SEED_)\n\nfor fold_n, (train_index, valid_index) in enumerate(kf.split(train_signal, train_target)):\n    print(f'BEGIN FOLD:     {fold_n} -------\\n\\n\\n')\n    \n    model = Unet_model()\n    model_checkpoint = ModelCheckpoint(\"model\" + str(fold_n) + \".hdf5\",\n                                       save_best_only=True, verbose=0, monitor='val_loss', mode='min')\n\n    X_train, X_valid = train_signal[train_index,:,:], train_signal[valid_index,:,:]\n    y_train, y_valid = train_target[train_index,:,:], train_target[valid_index,:,:]\n\n    callBacks = [early_stopping, model_checkpoint, lr_schedule, macroF1(model, X_valid, y_valid)]\n\n    history = model.fit(\n        X_train, y_train,\n        epochs = n_epoch,\n        callbacks = callBacks,\n        verbose = 1,\n        validation_data = (X_valid, y_valid)\n    )\n    print(f'\\n\\n\\nENDED FOLD:     {fold_n} -------\\n\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npred = np.zeros((test_signal.shape[0], test_signal.shape[1], n_classes))\nfor fold_ in range(folds):\n    model = load_trained_model(\"model\" + str(fold_) + \".hdf5\")\n    pred += model.predict(test_signal)/folds\n\n\nnp.save(\"pred.npy\", pred, allow_pickle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\n\nsub['open_channels'] = np.argmax(pred, axis = 2).reshape(-1)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}