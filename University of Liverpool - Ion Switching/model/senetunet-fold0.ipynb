{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0-rc2\r\n",
      "  Downloading tensorflow-2.2.0rc2-cp36-cp36m-manylinux2010_x86_64.whl (516.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 516.1 MB 23 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (3.11.3)\r\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\r\n",
      "  Downloading tensorboard-2.2.0-py3-none-any.whl (2.8 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 33.1 MB/s \r\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0rc0\r\n",
      "  Downloading tensorflow_estimator-2.2.0rc0-py2.py3-none-any.whl (454 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 454 kB 19.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (1.18.2)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (0.9.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (3.2.0)\r\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (0.34.2)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (1.1.0)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (1.27.2)\r\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (1.4.1)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (1.1.0)\r\n",
      "Collecting gast==0.3.3\r\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (1.11.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (1.14.0)\r\n",
      "Collecting astunparse==1.6.3\r\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (0.2.0)\r\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.2.0-rc2) (2.10.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow==2.2.0-rc2) (46.1.3.post20200330)\r\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 777 kB 40.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (3.2.1)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (1.12.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (2.22.0)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (1.0.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (4.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (0.2.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (4.0.0)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (1.24.3)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (2019.11.28)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (1.3.0)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc2) (3.1.0)\r\n",
      "Installing collected packages: tensorboard-plugin-wit, tensorboard, tensorflow-estimator, gast, astunparse, tensorflow\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.1.1\r\n",
      "    Uninstalling tensorboard-2.1.1:\r\n",
      "      Successfully uninstalled tensorboard-2.1.1\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\r\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\r\n",
      "  Attempting uninstall: gast\r\n",
      "    Found existing installation: gast 0.2.2\r\n",
      "    Uninstalling gast-0.2.2:\r\n",
      "      Successfully uninstalled gast-0.2.2\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.1.0\r\n",
      "    Uninstalling tensorflow-2.1.0:\r\n",
      "      Successfully uninstalled tensorflow-2.1.0\r\n",
      "Successfully installed astunparse-1.6.3 gast-0.3.3 tensorboard-2.2.0 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0rc2 tensorflow-estimator-2.2.0rc0\r\n",
      "Collecting tensorflow-addons\r\n",
      "  Downloading tensorflow_addons-0.9.1-cp36-cp36m-manylinux2010_x86_64.whl (1.0 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 2.0 kB/s \r\n",
      "\u001b[?25hCollecting typeguard>=2.7\r\n",
      "  Downloading typeguard-2.7.1-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: typeguard, tensorflow-addons\r\n",
      "Successfully installed tensorflow-addons-0.9.1 typeguard-2.7.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2.0-rc2\n",
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "import math\n",
    "# from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import os\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.python.keras.engine import training\n",
    "import collections\n",
    "\n",
    "LEN_ = 4000\n",
    "n_epoch = 1000\n",
    "batch_size = 64\n",
    "STEP_ = LEN_\n",
    "folds = 5\n",
    "learning_rate = 0.001\n",
    "SEED_ = 987654321\n",
    "patience = 20\n",
    "FOLD_ = 0\n",
    "n_classes = 11\n",
    "\n",
    "lr_schedule = LearningRateScheduler(\n",
    "    PolynomialDecay(\n",
    "      initial_learning_rate = learning_rate,\n",
    "      decay_steps = n_epoch,\n",
    "      power = 2,\n",
    "      end_learning_rate = 0.0\n",
    "    )\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience = patience, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBetaScore(tf.keras.metrics.Metric):\n",
    "    \"\"\"Computes F-Beta score.\n",
    "    It is the weighted harmonic mean of precision\n",
    "    and recall. Output range is [0, 1]. Works for\n",
    "    both multi-class and multi-label classification.\n",
    "    F-Beta = (1 + beta^2) * (prec * recall) / ((beta^2 * prec) + recall)\n",
    "    Args:\n",
    "        num_classes: Number of unique classes in the dataset.\n",
    "        average: Type of averaging to be performed on data.\n",
    "            Acceptable values are `None`, `micro`, `macro` and\n",
    "            `weighted`. Default value is None.\n",
    "        beta: Determines the weight of precision and recall\n",
    "            in harmonic mean. Determines the weight given to the\n",
    "            precision and recall. Default value is 1.\n",
    "        threshold: Elements of `y_pred` greater than threshold are\n",
    "            converted to be 1, and the rest 0. If threshold is\n",
    "            None, the argmax is converted to 1, and the rest 0.\n",
    "    Returns:\n",
    "        F-Beta Score: float\n",
    "    Raises:\n",
    "        ValueError: If the `average` has values other than\n",
    "        [None, micro, macro, weighted].\n",
    "        ValueError: If the `beta` value is less than or equal\n",
    "        to 0.\n",
    "    `average` parameter behavior:\n",
    "        None: Scores for each class are returned\n",
    "        micro: True positivies, false positives and\n",
    "            false negatives are computed globally.\n",
    "        macro: True positivies, false positives and\n",
    "            false negatives are computed for each class\n",
    "            and their unweighted mean is returned.\n",
    "        weighted: Metrics are computed for each class\n",
    "            and returns the mean weighted by the\n",
    "            number of true instances in each class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 shape = [LEN_, n_classes],\n",
    "                 average=None,\n",
    "                 beta=1.0,\n",
    "                 threshold=None,\n",
    "                 name='fbeta_score',\n",
    "                 dtype=tf.float32):\n",
    "        super(FBetaScore, self).__init__(name=name)\n",
    "\n",
    "        if average not in (None, 'micro', 'macro', 'weighted'):\n",
    "            raise ValueError(\"Unknown average type. Acceptable values \"\n",
    "                             \"are: [None, micro, macro, weighted]\")\n",
    "\n",
    "        if not isinstance(beta, float):\n",
    "            raise TypeError(\"The value of beta should be a python float\")\n",
    "\n",
    "        if beta <= 0.0:\n",
    "            raise ValueError(\"beta value should be greater than zero\")\n",
    "\n",
    "        if threshold is not None:\n",
    "            if not isinstance(threshold, float):\n",
    "                raise TypeError(\n",
    "                    \"The value of threshold should be a python float\")\n",
    "            if threshold > 1.0 or threshold <= 0.0:\n",
    "                raise ValueError(\"threshold should be between 0 and 1\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.average = average\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "        self.axis = None\n",
    "        self.init_shape = shape\n",
    "        \n",
    "        if self.average != 'micro':\n",
    "            self.axis = 0\n",
    "            # self.init_shape = [shape]\n",
    "\n",
    "        def _zero_wt_init(name):\n",
    "            return self.add_weight(\n",
    "                name,\n",
    "                shape=self.init_shape,\n",
    "                initializer='zeros',\n",
    "                dtype=self.dtype)\n",
    "\n",
    "        self.true_positives = _zero_wt_init('true_positives')\n",
    "        self.false_positives = _zero_wt_init('false_positives')\n",
    "        self.false_negatives = _zero_wt_init('false_negatives')\n",
    "        self.weights_intermediate = _zero_wt_init('weights_intermediate')\n",
    "\n",
    "    # TODO: Add sample_weight support, currently it is\n",
    "    # ignored during calculations.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if self.threshold is None:\n",
    "            threshold = tf.reduce_max(y_pred, axis=-1, keepdims=True)\n",
    "            # make sure [0, 0, 0] doesn't become [1, 1, 1]\n",
    "            # Use abs(x) > eps, instead of x != 0 to check for zero\n",
    "            y_pred = tf.logical_and(y_pred >= threshold,\n",
    "                                    tf.abs(y_pred) > 1e-12)\n",
    "        else:\n",
    "            y_pred = y_pred > self.threshold\n",
    "\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_pred = tf.cast(y_pred, tf.int32)\n",
    "\n",
    "        def _count_non_zero(val):\n",
    "            non_zeros = tf.math.count_nonzero(val, axis=self.axis)\n",
    "            return tf.cast(non_zeros, self.dtype)\n",
    "\n",
    "        self.true_positives.assign_add(_count_non_zero(y_pred * y_true))\n",
    "        self.false_positives.assign_add(_count_non_zero(y_pred * (y_true - 1)))\n",
    "        self.false_negatives.assign_add(_count_non_zero((y_pred - 1) * y_true))\n",
    "        self.weights_intermediate.assign_add(_count_non_zero(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        precision = tf.math.divide_no_nan(\n",
    "            self.true_positives, self.true_positives + self.false_positives)\n",
    "        recall = tf.math.divide_no_nan(\n",
    "            self.true_positives, self.true_positives + self.false_negatives)\n",
    "\n",
    "        mul_value = precision * recall\n",
    "        add_value = (tf.math.square(self.beta) * precision) + recall\n",
    "        mean = (tf.math.divide_no_nan(mul_value, add_value))\n",
    "        f1_score = mean * (1 + tf.math.square(self.beta))\n",
    "\n",
    "        if self.average == 'weighted':\n",
    "            weights = tf.math.divide_no_nan(\n",
    "                self.weights_intermediate,\n",
    "                tf.reduce_sum(self.weights_intermediate))\n",
    "            f1_score = tf.reduce_sum(f1_score * weights)\n",
    "\n",
    "        elif self.average is not None:  # [micro, macro]\n",
    "            f1_score = tf.reduce_mean(f1_score)\n",
    "\n",
    "        return f1_score\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Returns the serializable config of the metric.\"\"\"\n",
    "\n",
    "        config = {\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"average\": self.average,\n",
    "            \"beta\": self.beta,\n",
    "        }\n",
    "\n",
    "        if self.threshold is not None:\n",
    "            config[\"threshold\"] = self.threshold\n",
    "\n",
    "        base_config = super(FBetaScore, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(tf.zeros(self.init_shape, self.dtype))\n",
    "        self.false_positives.assign(tf.zeros(self.init_shape, self.dtype))\n",
    "        self.false_negatives.assign(tf.zeros(self.init_shape, self.dtype))\n",
    "        self.weights_intermediate.assign(tf.zeros(self.init_shape, self.dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(Activation):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "get_custom_objects().update({'Mish': Mish(mish)})\n",
    "\n",
    "class macroF1(Callback):\n",
    "    def __init__(self, model, inputs, targets, step = 10):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "        self.step = step\n",
    "\n",
    "    def on_train_begin(self, logs={}):    \n",
    "            # Initialization code    \n",
    "            self.epochs = 0    \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.epochs += 1   \n",
    "        if self.epochs % self.step == 0:  \n",
    "          pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "          f1_val = f1_score(self.targets, pred, average=\"macro\")\n",
    "          print(\"Step: {}  --  val_f1_macro_score: {}\".format(self.epochs, f1_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_params(**params):\n",
    "    axis = 2\n",
    "    default_bn_params = {\n",
    "        'axis': axis,\n",
    "        'epsilon': 9.999999747378752e-06,\n",
    "    }\n",
    "    default_bn_params.update(params)\n",
    "    return default_bn_params\n",
    "\n",
    "\n",
    "def get_num_channels(tensor):\n",
    "    channels_axis = 2\n",
    "    return backend.int_shape(tensor)[channels_axis]\n",
    "\n",
    "def get_conv_params(**params):\n",
    "    default_conv_params = {\n",
    "        'kernel_initializer': 'he_uniform',\n",
    "        'use_bias': False,\n",
    "        'padding': 'valid',\n",
    "    }\n",
    "    default_conv_params.update(params)\n",
    "    return default_conv_params\n",
    "\n",
    "def handle_block_names(stage, block):\n",
    "    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n",
    "    conv_name = name_base + 'conv'\n",
    "    bn_name = name_base + 'bn'\n",
    "    relu_name = name_base + 'relu'\n",
    "    sc_name = name_base + 'sc'\n",
    "    return conv_name, bn_name, relu_name, sc_name\n",
    "\n",
    "def expand_dims(x, channels_axis):\n",
    "    return x[:, None, :]\n",
    "\n",
    "ModelParams = collections.namedtuple(\n",
    "    'ModelParams',\n",
    "    ['model_name', 'repetitions', 'residual_block', 'attention']\n",
    ")\n",
    "\n",
    "def ChannelSE(reduction=16):\n",
    "    \"\"\"\n",
    "    Squeeze and Excitation block, reimplementation inspired by\n",
    "        https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py\n",
    "    Args:\n",
    "        reduction: channels squeeze factor\n",
    "    \"\"\"\n",
    "#     backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "    channels_axis = 2\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        channels = tf.keras.backend.int_shape(input_tensor)[channels_axis]\n",
    "\n",
    "        # get number of channels/filters\n",
    "        x = input_tensor\n",
    "\n",
    "        # squeeze and excitation block in PyTorch style with\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        x = layers.Lambda(expand_dims, arguments={'channels_axis': channels_axis})(x)\n",
    "        x = layers.Conv1D(channels // reduction, 1, kernel_initializer='he_uniform')(x)\n",
    "        x = layers.Activation('Mish')(x)\n",
    "        x = layers.Conv1D(channels, 1, kernel_initializer='he_uniform')(x)\n",
    "        x = layers.Activation('sigmoid')(x)\n",
    "\n",
    "        # apply attention\n",
    "        x = layers.Multiply()([input_tensor, x])\n",
    "\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "def residual_conv_block(filters, stage, block, strides=(1, 1), attention=None, cut='pre'):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        cut: one of 'pre', 'post'. used to decide where skip connection is taken\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        # get params and names of layers\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = layers.BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = layers.Activation('Mish', name=relu_name + '1')(x)\n",
    "\n",
    "        # defining shortcut connection\n",
    "        if cut == 'pre':\n",
    "            shortcut = input_tensor\n",
    "        elif cut == 'post':\n",
    "            shortcut = layers.Conv1D(filters, 1, name=sc_name, strides=strides, **conv_params)(x)\n",
    "        else:\n",
    "            raise ValueError('Cut type not in [\"pre\", \"post\"]')\n",
    "\n",
    "        # continue with convolution layers\n",
    "        x = layers.ZeroPadding1D(padding=1)(x)\n",
    "        x = layers.Conv1D(filters, 3, strides=strides, name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = layers.BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = layers.Activation('Mish', name=relu_name + '2')(x)\n",
    "        x = layers.ZeroPadding1D(padding=1)(x)\n",
    "        x = layers.Conv1D(filters, 3, name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        # use attention block if defined\n",
    "        if attention is not None:\n",
    "            x = attention(x)\n",
    "\n",
    "        # add residual connection\n",
    "        x = layers.Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#   Residual Model Builder\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def ResNet(model_params, input_shape=None, include_top=False,\n",
    "           classes=11, **kwargs):\n",
    "    \"\"\"Instantiates the ResNet, SEResNet architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    Args:\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    Returns:\n",
    "        A Keras model instance.\n",
    "    Raises:\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    # choose residual block type\n",
    "    ResidualBlock = model_params.residual_block\n",
    "    if model_params.attention:\n",
    "        Attention = model_params.attention(**kwargs)\n",
    "    else:\n",
    "        Attention = None\n",
    "\n",
    "    # get parameters for model layers\n",
    "    no_scale_bn_params = get_bn_params(scale=False)\n",
    "    bn_params = get_bn_params()\n",
    "    conv_params = get_conv_params()\n",
    "    init_filters = 64\n",
    "\n",
    "    # resnet bottom\n",
    "    x = layers.BatchNormalization(name='bn_data', **no_scale_bn_params)(input_layer)\n",
    "    x = layers.ZeroPadding1D(padding=3)(x)\n",
    "    x = layers.Conv1D(init_filters, 7, strides=2, name='conv0', **conv_params)(x)\n",
    "    x = layers.BatchNormalization(name='bn0', **bn_params)(x)\n",
    "    x = layers.Activation('Mish', name='relu0')(x)\n",
    "    x = layers.ZeroPadding1D(padding=1)(x)\n",
    "    x = layers.MaxPooling1D(3, strides=2, padding='valid', name='pooling0')(x)\n",
    "\n",
    "    # resnet body\n",
    "    for stage, rep in enumerate(model_params.repetitions):\n",
    "        for block in range(rep):\n",
    "\n",
    "            filters = init_filters * (2 ** stage)\n",
    "\n",
    "            # first block of first stage without strides because we have maxpooling before\n",
    "            if block == 0 and stage == 0:\n",
    "                x = ResidualBlock(filters, stage, block, strides=1,\n",
    "                                  cut='post', attention=Attention)(x)\n",
    "\n",
    "            elif block == 0:\n",
    "                x = ResidualBlock(filters, stage, block, strides=2,\n",
    "                                  cut='post', attention=Attention)(x)\n",
    "\n",
    "            else:\n",
    "                x = ResidualBlock(filters, stage, block, strides=1,\n",
    "                                  cut='pre', attention=Attention)(x)\n",
    "\n",
    "    x = layers.BatchNormalization(name='bn1', **bn_params)(x)\n",
    "    x = layers.Activation('Mish', name='relu1')(x)\n",
    "\n",
    "    # Create model.\n",
    "    model = training.Model(input_layer, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def SEResNet34(input_shape= (LEN_,1), classes=11, **kwargs):\n",
    "    return ResNet(\n",
    "        ModelParams('seresnet34', (3, 4, 6, 3), residual_conv_block, ChannelSE),\n",
    "        input_shape=input_shape,\n",
    "        classes=classes,\n",
    "        **kwargs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2dBn(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        activation='Mish',\n",
    "        strides = 1,\n",
    "        kernel_initializer='he_uniform',\n",
    "        padding='valid'):\n",
    "    \"\"\"Extension of Conv2D layer with batchnorm\"\"\"\n",
    "\n",
    "    bn_axis = 2\n",
    "\n",
    "    def wrapper(input_tensor):\n",
    "\n",
    "        x = layers.Conv1D(\n",
    "              filters,\n",
    "              kernel_size,\n",
    "              strides=strides,\n",
    "              padding=padding)(input_tensor)\n",
    "        x = layers.BatchNormalization(axis=bn_axis)(x)\n",
    "        x = layers.Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def Conv3x3BnMish(filters, use_batchnorm, name=None):\n",
    "\n",
    "    def wrapper(input_tensor):\n",
    "        return Conv2dBn(\n",
    "            filters,\n",
    "            kernel_size=3,\n",
    "            activation='Mish',\n",
    "            kernel_initializer='he_uniform',\n",
    "            padding='same',\n",
    "        )(input_tensor)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def DecoderUpsamplingX2Block(filters, stage, use_batchnorm=False):\n",
    "    up_name = 'decoder_stage{}_upsampling'.format(stage)\n",
    "    conv1_name = 'decoder_stage{}a'.format(stage)\n",
    "    conv2_name = 'decoder_stage{}b'.format(stage)\n",
    "    concat_name = 'decoder_stage{}_concat'.format(stage)\n",
    "\n",
    "    concat_axis = 2\n",
    "    \n",
    "    def wrapper(input_tensor, skip=None):\n",
    "        x = layers.UpSampling1D(size=2, name=up_name)(input_tensor)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = layers.Concatenate(axis=concat_axis, name=concat_name)([x, skip])\n",
    "\n",
    "        x = Conv3x3BnMish(filters, use_batchnorm, name=conv1_name)(x)\n",
    "        x = Conv3x3BnMish(filters, use_batchnorm, name=conv2_name)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "feature_layer = ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0')\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  Unet Decoder\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_unet(\n",
    "        backbone,\n",
    "        decoder_block = DecoderUpsamplingX2Block,\n",
    "        skip_connection_layers = feature_layer,\n",
    "        decoder_filters = (512, 256, 128, 64, 32, 16),\n",
    "        n_upsample_blocks=5,\n",
    "        classes=11,\n",
    "        activation='sigmoid',\n",
    "        use_batchnorm=True,\n",
    "):\n",
    "    input_ = backbone.input\n",
    "    x = backbone.output\n",
    "\n",
    "    # extract skip connections\n",
    "    skips = ([backbone.get_layer(name=i).output if isinstance(i, str)\n",
    "                  else backbone.get_layer(index=i).output for i in skip_connection_layers])\n",
    "    # add center block if previous operation was maxpooling (for vgg models)\n",
    "    if isinstance(backbone.layers[-1], layers.MaxPooling1D):\n",
    "        x = Conv3x3BnReLU(512, use_batchnorm, name='center_block1')(x)\n",
    "        x = Conv3x3BnReLU(512, use_batchnorm, name='center_block2')(x)\n",
    "\n",
    "    # building decoder blocks\n",
    "    for i in range(n_upsample_blocks):\n",
    "\n",
    "        if i < len(skips):\n",
    "            skip = skips[i]\n",
    "        else:\n",
    "            skip = None\n",
    "\n",
    "        x = decoder_block(decoder_filters[i], stage=i, use_batchnorm=use_batchnorm)(x, skip)\n",
    "\n",
    "    # model head (define number of output classes)\n",
    "    x = layers.Conv1D(\n",
    "        filters=classes,\n",
    "        kernel_size=3,\n",
    "        padding='same',\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        name='final_conv',\n",
    "    )(x)\n",
    "    x = layers.Activation(activation, name=activation)(x)\n",
    "\n",
    "    # create keras model instance\n",
    "    model = training.Model(input_, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def SEResNetUnet(include_top=False, input_shape = (LEN_, 1)):\n",
    "    backbone = SEResNet34()\n",
    "    model = build_unet(backbone)\n",
    "    model.compile(Adam(learning_rate = learning_rate), loss = [tfa.losses.SigmoidFocalCrossEntropy()],#'categorical_crossentropy', \n",
    "          metrics=[FBetaScore(n_classes, average = \"macro\")])\n",
    "\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def load_trained_model(weights_path):\n",
    "    modelf = SEResNetUnet()\n",
    "    modelf.load_weights(weights_path)\n",
    "    \n",
    "    return(modelf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter used: 10292826\n"
     ]
    }
   ],
   "source": [
    "model = SEResNetUnet()\n",
    "print(f\"Number of parameter used: {model.count_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/clean-removal-of-data-drift/train.csv\")\n",
    "_ = pd.read_csv(\"../input/liverpool-ion-switching/train.csv\")\n",
    "train['time'] = _['time']\n",
    "del _\n",
    "\n",
    "test = pd.read_csv(\"../input/clean-removal-of-data-drift/test.csv\")\n",
    "_ = pd.read_csv(\"../input/liverpool-ion-switching/test.csv\")\n",
    "test['time'] = _['time']\n",
    "del _\n",
    "\n",
    "\n",
    "train_mean = train.signal.mean()\n",
    "train_sigma = train.signal.std()\n",
    "\n",
    "train['signal'] = (train['signal'] - train_mean)/train_sigma\n",
    "test['signal'] = (test['signal'] - train_mean)/train_sigma\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train.shape[0] % LEN_ == 0, 'Check dimension of LEN_ in comparison to size of train. It need to be a multiple!'\n",
    "\n",
    "train_signal = train['signal'].values.reshape(-1, LEN_, 1)\n",
    "train_target = pd.get_dummies(train['channel']).values.reshape(-1, LEN_, n_classes)\n",
    "\n",
    "test_signal = test[\"signal\"].values.reshape(-1,LEN_,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN FOLD:     0 -------\n",
      "\n",
      "\n",
      "\n",
      "Step: 10  --  val_f1_macro_score: 0.49456155388787976\n",
      "Step: 20  --  val_f1_macro_score: 0.7066749310322747\n",
      "Step: 30  --  val_f1_macro_score: 0.759301674983666\n",
      "Step: 40  --  val_f1_macro_score: 0.8025299989351132\n",
      "Step: 50  --  val_f1_macro_score: 0.8601029596531508\n",
      "Step: 60  --  val_f1_macro_score: 0.8929693344061096\n",
      "Step: 70  --  val_f1_macro_score: 0.8938505025083668\n",
      "Step: 80  --  val_f1_macro_score: 0.898381203643515\n",
      "Step: 90  --  val_f1_macro_score: 0.8945131938852131\n",
      "Step: 100  --  val_f1_macro_score: 0.9157512979223014\n",
      "Step: 110  --  val_f1_macro_score: 0.8129931776695816\n",
      "Step: 120  --  val_f1_macro_score: 0.9209086649379735\n",
      "Step: 130  --  val_f1_macro_score: 0.9092424851824895\n",
      "Step: 140  --  val_f1_macro_score: 0.9083205288271156\n",
      "Step: 150  --  val_f1_macro_score: 0.9207508352030402\n",
      "Step: 160  --  val_f1_macro_score: 0.900805448164816\n",
      "Step: 170  --  val_f1_macro_score: 0.9158767443673731\n",
      "Step: 180  --  val_f1_macro_score: 0.9293143896105907\n",
      "Step: 190  --  val_f1_macro_score: 0.910537626416986\n",
      "Step: 200  --  val_f1_macro_score: 0.9220509651205507\n",
      "Step: 210  --  val_f1_macro_score: 0.9098380935479627\n",
      "Step: 220  --  val_f1_macro_score: 0.905578282546522\n",
      "Step: 230  --  val_f1_macro_score: 0.9234289492099369\n",
      "Step: 240  --  val_f1_macro_score: 0.9277059928200451\n",
      "Step: 250  --  val_f1_macro_score: 0.9121816976099989\n",
      "Step: 260  --  val_f1_macro_score: 0.932670524871896\n",
      "Step: 270  --  val_f1_macro_score: 0.3684815658861247\n",
      "Step: 280  --  val_f1_macro_score: 0.47891590948506374\n",
      "Step: 290  --  val_f1_macro_score: 0.9265242023371446\n",
      "Step: 300  --  val_f1_macro_score: 0.92978501226459\n",
      "Step: 310  --  val_f1_macro_score: 0.9285989215899949\n",
      "Step: 320  --  val_f1_macro_score: 0.9324658132870163\n",
      "Step: 330  --  val_f1_macro_score: 0.930460630277659\n",
      "Step: 340  --  val_f1_macro_score: 0.9333379945890742\n",
      "Step: 350  --  val_f1_macro_score: 0.931004855819677\n",
      "Step: 360  --  val_f1_macro_score: 0.9312663198780009\n",
      "Step: 370  --  val_f1_macro_score: 0.9322663219296863\n",
      "Step: 380  --  val_f1_macro_score: 0.9295894582021177\n",
      "Step: 390  --  val_f1_macro_score: 0.9341479356203043\n",
      "Step: 400  --  val_f1_macro_score: 0.9102596187636038\n",
      "Step: 410  --  val_f1_macro_score: 0.9253087444917003\n",
      "Step: 420  --  val_f1_macro_score: 0.9344591284982162\n",
      "Step: 430  --  val_f1_macro_score: 0.9335111924481846\n",
      "Step: 440  --  val_f1_macro_score: 0.9316979157204913\n",
      "Step: 450  --  val_f1_macro_score: 0.9350578091298217\n",
      "Step: 460  --  val_f1_macro_score: 0.9330155339192725\n",
      "Step: 470  --  val_f1_macro_score: 0.9353875301092017\n",
      "Step: 480  --  val_f1_macro_score: 0.9288125475862454\n",
      "Step: 490  --  val_f1_macro_score: 0.9338657918180452\n",
      "Step: 500  --  val_f1_macro_score: 0.9354875055027345\n",
      "Step: 510  --  val_f1_macro_score: 0.9353107780123202\n",
      "Step: 520  --  val_f1_macro_score: 0.9355000198295369\n",
      "Step: 530  --  val_f1_macro_score: 0.9316549798102066\n",
      "Step: 540  --  val_f1_macro_score: 0.9334211433495462\n",
      "Step: 550  --  val_f1_macro_score: 0.9337439789555443\n",
      "Step: 560  --  val_f1_macro_score: 0.9347472954174545\n",
      "Step: 570  --  val_f1_macro_score: 0.9328258495452588\n",
      "Step: 580  --  val_f1_macro_score: 0.9349551353021226\n",
      "Step: 590  --  val_f1_macro_score: 0.9357296395917274\n",
      "Step: 600  --  val_f1_macro_score: 0.9343738480084969\n",
      "Step: 610  --  val_f1_macro_score: 0.935201821517259\n",
      "Step: 620  --  val_f1_macro_score: 0.9336036417142481\n",
      "Step: 630  --  val_f1_macro_score: 0.9349826843246238\n",
      "Step: 640  --  val_f1_macro_score: 0.9348816037415287\n",
      "Step: 650  --  val_f1_macro_score: 0.9353590015313317\n",
      "Step: 660  --  val_f1_macro_score: 0.9299250017560592\n",
      "Step: 670  --  val_f1_macro_score: 0.9330315069901801\n",
      "Step: 680  --  val_f1_macro_score: 0.9358117942331432\n",
      "Step: 690  --  val_f1_macro_score: 0.9356227766327464\n",
      "Step: 700  --  val_f1_macro_score: 0.9347527066800084\n",
      "Step: 710  --  val_f1_macro_score: 0.9358588622050942\n",
      "Step: 720  --  val_f1_macro_score: 0.9355483279188319\n",
      "Step: 730  --  val_f1_macro_score: 0.9355351720932112\n",
      "Step: 740  --  val_f1_macro_score: 0.9354434945183104\n",
      "Step: 750  --  val_f1_macro_score: 0.9346657363699662\n",
      "Step: 760  --  val_f1_macro_score: 0.9354520027463585\n",
      "Step: 770  --  val_f1_macro_score: 0.935794998906502\n",
      "Step: 780  --  val_f1_macro_score: 0.9352509336174879\n",
      "Step: 790  --  val_f1_macro_score: 0.9356155716987661\n",
      "Step: 800  --  val_f1_macro_score: 0.9356480766738945\n",
      "Step: 810  --  val_f1_macro_score: 0.9355525133872394\n",
      "Step: 820  --  val_f1_macro_score: 0.9354864372343239\n",
      "Step: 830  --  val_f1_macro_score: 0.9354349117339029\n",
      "Step: 840  --  val_f1_macro_score: 0.9352004119814602\n",
      "Step: 850  --  val_f1_macro_score: 0.9349859688437178\n",
      "Step: 860  --  val_f1_macro_score: 0.9348819533622259\n",
      "Step: 870  --  val_f1_macro_score: 0.9342541383236569\n",
      "Step: 880  --  val_f1_macro_score: 0.9347060385510879\n",
      "Step: 890  --  val_f1_macro_score: 0.9348202596573318\n",
      "Step: 900  --  val_f1_macro_score: 0.9348185898121097\n",
      "Step: 910  --  val_f1_macro_score: 0.9347028934183759\n",
      "Step: 920  --  val_f1_macro_score: 0.9348037769922076\n",
      "Step: 930  --  val_f1_macro_score: 0.9345685934660829\n",
      "Step: 940  --  val_f1_macro_score: 0.9345082583371663\n",
      "Step: 950  --  val_f1_macro_score: 0.9345265257740976\n",
      "Step: 960  --  val_f1_macro_score: 0.9345290251798218\n",
      "Step: 970  --  val_f1_macro_score: 0.934506301821569\n",
      "Step: 980  --  val_f1_macro_score: 0.9344736782122258\n",
      "Step: 990  --  val_f1_macro_score: 0.9346093264514479\n",
      "Step: 1000  --  val_f1_macro_score: 0.9345699384298892\n",
      "\n",
      "\n",
      "\n",
      "ENDED FOLD:     0 -------\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 1h 23min 14s, sys: 22min 51s, total: 1h 46min 5s\n",
      "Wall time: 1h 44min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "kf = KFold(n_splits = folds, shuffle = True, random_state = SEED_)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf.split(train_signal, train_target)):\n",
    "    if fold_n == FOLD_:\n",
    "        print(f'BEGIN FOLD:     {fold_n} -------\\n\\n\\n')\n",
    "\n",
    "        model = SEResNetUnet()\n",
    "        model_checkpoint = ModelCheckpoint(\"model\" + str(fold_n) + \".hdf5\",\n",
    "                                           save_best_only=True, verbose=0, monitor='val_fbeta_score', mode='max')\n",
    "\n",
    "        X_train, X_valid = train_signal[train_index,:,:], train_signal[valid_index,:,:]\n",
    "        y_train, y_valid = train_target[train_index,:,:], train_target[valid_index,:,:]\n",
    "\n",
    "        callBacks = [model_checkpoint, lr_schedule, macroF1(model, X_valid, y_valid)] #early_stopping, \n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs = n_epoch,\n",
    "            batch_size = batch_size,\n",
    "            callbacks = callBacks,\n",
    "            verbose = 0,\n",
    "            validation_data = (X_valid, y_valid)\n",
    "        )\n",
    "        print(f'\\n\\n\\nENDED FOLD:     {fold_n} -------\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.78 s, sys: 264 ms, total: 7.05 s\n",
      "Wall time: 7.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pred = np.zeros((test_signal.shape[0], test_signal.shape[1], n_classes))\n",
    "# for fold_ in range(folds):\n",
    "#     model = load_trained_model(\"model\" + str(fold_) + \".hdf5\")\n",
    "#     pred += model.predict(test_signal)/folds\n",
    "# np.save(\"pred.npy\", pred, allow_pickle = True)\n",
    "\n",
    "model = load_trained_model(\"model\" + str(FOLD_) + \".hdf5\")\n",
    "pred = model.predict(test_signal)\n",
    "\n",
    "np.save(\"pred\" + str(FOLD_) + \".npy\", pred, allow_pickle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\n",
    "\n",
    "sub['open_channels'] = np.argmax(pred, axis = 2).reshape(-1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
